{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1569a849",
   "metadata": {},
   "source": [
    "# Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3342d0cc",
   "metadata": {},
   "source": [
    "O Gradient Boosting é uma técnica de aprendizado de máquina baseada em ensemble que constrói um modelo preditivo forte a partir de uma coleção de modelos preditivos fracos. A ideia central é combinar vários estimadores simples (geralmente árvores de decisão rasas) sequencialmente, onde cada novo modelo é treinado para corrigir os erros do modelo anterior.\n",
    "\n",
    "## Funcionamento:\n",
    "- **Inicialização:** Inicialmente, o algoritmo ajusta um modelo simples aos dados de treinamento. Isso pode ser um modelo constante (por exemplo, a média das respostas) ou um modelo simples, como uma árvore de decisão com poucas divisões.\n",
    "\n",
    "- **Iteração:** Em cada iteração subsequente, o algoritmo ajusta um novo modelo para os resíduos (diferença entre as previsões atuais e os rótulos verdadeiros) do modelo anterior. O objetivo é minimizar os resíduos residuais.\n",
    "\n",
    "- **Combinação de Modelos:** Os modelos são combinados aditivamente, o que significa que as previsões finais são obtidas somando as previsões de todos os modelos individuais.\n",
    "\n",
    "## Gradiente Descendente:\n",
    "O termo \"gradiente\" em Gradient Boosting refere-se ao uso do gradiente da função de perda (loss function) em relação às previsões do modelo. O algoritmo ajusta cada modelo subsequente na direção do gradiente negativo da função de perda, tentando reduzir a perda global.\n",
    "\n",
    "## Regularização:\n",
    "Para evitar overfitting, é comum aplicar regularização durante o treinamento do Gradient Boosting. Isso pode ser feito limitando o número de iterações (número de modelos no ensemble), ajustando a taxa de aprendizado (learning rate) ou usando árvores de decisão rasas (evitando árvores profundas que podem se ajustar muito bem aos dados de treinamento).\n",
    "\n",
    "## Implementação no scikit-learn:\n",
    "O GradientBoostingClassifier do scikit-learn é uma implementação eficiente e fácil de usar do Gradient Boosting para tarefas de classificação. Ele permite configurar diversos hiperparâmetros, como o número de estimadores (número de modelos no ensemble), a profundidade máxima das árvores de decisão, a taxa de aprendizado, entre outros.\n",
    "\n",
    "## Vantagens:\n",
    "- **Poder Preditivo:** O Gradient Boosting geralmente produz modelos com alta precisão devido à sua capacidade de ajustar modelos complexos.\n",
    "\n",
    "- **Flexibilidade:** Pode ser aplicado a uma variedade de problemas de aprendizado supervisionado e é robusto a outliers e ruído nos dados.\n",
    "\n",
    "## Limitações:\n",
    "- **Sensibilidade a Hiperparâmetros:** O desempenho do Gradient Boosting pode depender fortemente da escolha dos hiperparâmetros, como a taxa de aprendizado e o número de estimadores.\n",
    "\n",
    "- **Computacionalmente Intensivo:** Treinar um Gradient Boosting pode ser computacionalmente caro, especialmente com um grande número de estimadores.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42a4ec7",
   "metadata": {},
   "source": [
    "# Parâmetros para o `GradientBoostingClassifier` do scikit-learn:\n",
    "\n",
    "1. `loss`: A função de perda a ser otimizada. Pode ser \"deviance\" para classificação com perda de desvio ou \"exponential\" para classificação com perda exponencial.\n",
    "\n",
    "2. `learning_rate`: Taxa de aprendizado, controla a contribuição de cada árvore na correção dos erros cometidos pelas árvores anteriores. Um valor menor geralmente resulta em um modelo mais robusto, mas requer mais árvores no ensemble.\n",
    "\n",
    "3. `n_estimators`: O número de árvores de decisão no ensemble.\n",
    "\n",
    "4. `subsample`: A fração de amostras a serem usadas para ajustar cada árvore. Um valor menor resulta em um modelo mais robusto, reduzindo o overfitting.\n",
    "\n",
    "5. `criterion`: A função para medir a qualidade de uma divisão. Pode ser \"friedman_mse\" para o erro médio quadrático de Friedman com melhoria potencial ou \"mse\" para o erro médio quadrático.\n",
    "\n",
    "6. `min_samples_split`: O número mínimo de amostras necessárias para dividir um nó interno.\n",
    "\n",
    "7. `min_samples_leaf`: O número mínimo de amostras necessárias para estar em um nó folha.\n",
    "\n",
    "8. `min_weight_fraction_leaf`: A fração mínima ponderada do total de pesos das amostras de entrada necessárias para estar em um nó folha.\n",
    "\n",
    "9. `max_depth`: A profundidade máxima das árvores de decisão.\n",
    "\n",
    "10. `min_impurity_decrease`: Um nó será dividido se essa divisão induzir uma diminuição da impureza maior ou igual a esse valor.\n",
    "\n",
    "11. `min_impurity_split`: Limiar de impureza para parar o crescimento da árvore. Um nó será dividido se sua impureza for maior que o limiar, caso contrário, será uma folha.\n",
    "\n",
    "12. `init`: O estimador inicial a ser usado. Por padrão, é uma árvore de decisão.\n",
    "\n",
    "13. `random_state`: Determina a semente usada pelo gerador de números aleatórios para garantir que os resultados sejam reproduzíveis.\n",
    "\n",
    "14. `max_features`: O número de features a serem consideradas ao procurar a melhor divisão. \n",
    "\n",
    "15. `verbose`: Controla a verbosidade da saída durante o ajuste.\n",
    "\n",
    "16. `max_leaf_nodes`: O número máximo de folhas permitidas em cada árvore.\n",
    "\n",
    "17. `warm_start`: Se definido como True, reutiliza a solução da chamada anterior para ajustar e adiciona mais estimadores ao conjunto de árvores existente. \n",
    "\n",
    "18. `presort`: Se deve pré-ordenar os dados para acelerar o processo de ajuste."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c834e4c",
   "metadata": {},
   "source": [
    "Para o `GradientBoostingRegressor` adiciona:\n",
    "\n",
    "`alpha`: O parâmetro de regularização L1 (somente para loss='huber')."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e810b25",
   "metadata": {},
   "source": [
    "Importando bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47f73480",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gsidiag as gd\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afb30182",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "from sklearn.metrics._plot.precision_recall_curve import PrecisionRecallDisplay\n",
    "from sklearn.metrics import average_precision_score, precision_recall_curve\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "from itertools import combinations\n",
    "from itertools import cycle\n",
    "import seaborn as sns\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1357b191",
   "metadata": {},
   "source": [
    "Definindo parâmetros para o uso do readDiag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0806f6b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2020010100', '2020010106', '2020010112', '2020010118', '2020010200', '2020010206']\n"
     ]
    }
   ],
   "source": [
    "#DIRdiag = \"/home/patrick/readDiag/data\"\n",
    "DIRdiag = \"/mnt/d/ftp1.cptec.inpe.br/pesquisa/das/joao.gerd/EXP18/GSI/dataout\"\n",
    "#D:\\ftp1.cptec.inpe.br\\pesquisa\\das\\joao.gerd\\EXP18\\GSI\\dataout\n",
    "varName = \"amsua\"\n",
    "varType = \"n15\"\n",
    "dateIni=\"2020010100\" \n",
    "dateFin=\"2020010206\" \n",
    "nHour = \"6\"          \n",
    "vminOMA = -2.0       \n",
    "vmaxOMA = 2.0        \n",
    "vminSTD = 0.0        \n",
    "vmaxSTD = 14.0       \n",
    "Level = 1000\n",
    "Lay = None           \n",
    "SingleL = \"All\" \n",
    "\n",
    "datei = datetime.strptime(str(dateIni), \"%Y%m%d%H\")\n",
    "datef = datetime.strptime(str(dateFin), \"%Y%m%d%H\")\n",
    "dates = [dates.strftime('%Y%m%d%H') for dates in pd.date_range(datei, datef,freq=\"6H\").tolist()]\n",
    "\n",
    "print(dates)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d45849",
   "metadata": {},
   "source": [
    "Encontrando e listando arquivos para serem usados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c313c705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/mnt/d/ftp1.cptec.inpe.br/pesquisa/das/joao.gerd/EXP18/GSI/dataout/2020010100/diag_amsua_n15_01.2020010100', '/mnt/d/ftp1.cptec.inpe.br/pesquisa/das/joao.gerd/EXP18/GSI/dataout/2020010106/diag_amsua_n15_01.2020010106', '/mnt/d/ftp1.cptec.inpe.br/pesquisa/das/joao.gerd/EXP18/GSI/dataout/2020010112/diag_amsua_n15_01.2020010112', '/mnt/d/ftp1.cptec.inpe.br/pesquisa/das/joao.gerd/EXP18/GSI/dataout/2020010118/diag_amsua_n15_01.2020010118', '/mnt/d/ftp1.cptec.inpe.br/pesquisa/das/joao.gerd/EXP18/GSI/dataout/2020010200/diag_amsua_n15_01.2020010200', '/mnt/d/ftp1.cptec.inpe.br/pesquisa/das/joao.gerd/EXP18/GSI/dataout/2020010206/diag_amsua_n15_01.2020010206']\n",
      "\n",
      "['/mnt/d/ftp1.cptec.inpe.br/pesquisa/das/joao.gerd/EXP18/GSI/dataout/2020010100/diag_amsua_n15_03.2020010100', '/mnt/d/ftp1.cptec.inpe.br/pesquisa/das/joao.gerd/EXP18/GSI/dataout/2020010106/diag_amsua_n15_03.2020010106', '/mnt/d/ftp1.cptec.inpe.br/pesquisa/das/joao.gerd/EXP18/GSI/dataout/2020010112/diag_amsua_n15_03.2020010112', '/mnt/d/ftp1.cptec.inpe.br/pesquisa/das/joao.gerd/EXP18/GSI/dataout/2020010118/diag_amsua_n15_03.2020010118', '/mnt/d/ftp1.cptec.inpe.br/pesquisa/das/joao.gerd/EXP18/GSI/dataout/2020010200/diag_amsua_n15_03.2020010200', '/mnt/d/ftp1.cptec.inpe.br/pesquisa/das/joao.gerd/EXP18/GSI/dataout/2020010206/diag_amsua_n15_03.2020010206']\n"
     ]
    }
   ],
   "source": [
    "paths, pathsc = [], []\n",
    "\n",
    "OuterL = \"01\"        \n",
    "[paths.append(DIRdiag+\"/\"+dt+\"/diag_amsua_n15_\"+OuterL+\".\"+dt) for dt in dates]\n",
    "\n",
    "OuterLc = \"03\"\n",
    "[pathsc.append(DIRdiag+\"/\"+dt+\"/diag_amsua_n15_\"+OuterLc+\".\"+dt) for dt in dates]\n",
    "\n",
    "print(paths)\n",
    "print(\"\")\n",
    "print(pathsc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6771b1b8",
   "metadata": {},
   "source": [
    "Lendo arquivos listado usando o readDiag e concatenando os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a2d6284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Aguarde, o tempo total estimado para a leitura dos arquivos é de 2 minutos e 0 segundos.\n",
      "\n",
      "Reading /mnt/d/ftp1.cptec.inpe.br/pesquisa/das/joao.gerd/EXP18/GSI/dataout/2020010100/diag_amsua_n15_01.2020010100\n",
      " \n",
      ">>> GSI DIAG <<<\n",
      " \n",
      "Reading /mnt/d/ftp1.cptec.inpe.br/pesquisa/das/joao.gerd/EXP18/GSI/dataout/2020010106/diag_amsua_n15_01.2020010106\n",
      " \n",
      ">>> GSI DIAG <<<\n",
      " \n",
      "Reading /mnt/d/ftp1.cptec.inpe.br/pesquisa/das/joao.gerd/EXP18/GSI/dataout/2020010112/diag_amsua_n15_01.2020010112\n",
      " \n",
      ">>> GSI DIAG <<<\n",
      " \n",
      "Reading /mnt/d/ftp1.cptec.inpe.br/pesquisa/das/joao.gerd/EXP18/GSI/dataout/2020010118/diag_amsua_n15_01.2020010118\n",
      " \n",
      ">>> GSI DIAG <<<\n",
      " \n",
      "Reading /mnt/d/ftp1.cptec.inpe.br/pesquisa/das/joao.gerd/EXP18/GSI/dataout/2020010200/diag_amsua_n15_01.2020010200\n",
      " \n",
      ">>> GSI DIAG <<<\n",
      " \n",
      "Reading /mnt/d/ftp1.cptec.inpe.br/pesquisa/das/joao.gerd/EXP18/GSI/dataout/2020010206/diag_amsua_n15_01.2020010206\n",
      " \n",
      ">>> GSI DIAG <<<\n",
      " \n",
      "[<gsidiag.__main__.read_diag object at 0x7fcd63812dd0>, <gsidiag.__main__.read_diag object at 0x7fcd6abdc510>, <gsidiag.__main__.read_diag object at 0x7fcd61f2f810>, <gsidiag.__main__.read_diag object at 0x7fcd5d649050>, <gsidiag.__main__.read_diag object at 0x7fcd5c0d8510>, <gsidiag.__main__.read_diag object at 0x7fcd57f2b510>]\n"
     ]
    }
   ],
   "source": [
    "# Define uma variável booleana 'read' como True para indicar que a leitura dos arquivos será realizada\n",
    "read = True\n",
    "\n",
    "# Verifica se 'read' é True para prosseguir com a leitura dos arquivos\n",
    "if read:        \n",
    "    # Inicializa uma lista vazia para armazenar os objetos gdf\n",
    "    gdf_list = []\n",
    "    # Imprime uma mensagem informando o tempo estimado necessário para a leitura dos arquivos\n",
    "    print(\"\")\n",
    "    print(\"Aguarde, o tempo total estimado para a leitura dos arquivos é de \"+\n",
    "          str(int((float(len(paths))*20 )/60))+\" minutos e \"+\n",
    "          str(int((float(len(paths))*20 )%60))+\" segundos.\")\n",
    "    print(\"\")\n",
    "    # Itera sobre os caminhos dos arquivos e seus caminhos de configuração correspondentes\n",
    "    for path, pathc in zip(paths,pathsc):\n",
    "        # Imprime uma mensagem indicando o arquivo que está sendo lido\n",
    "        print(\"Reading \"+path)\n",
    "        # Lê o arquivo usando a função read_diag do módulo gsidiag e armazena o objeto retornado em gdf\n",
    "        gdf = gd.read_diag(path,pathc)\n",
    "        # Adiciona o objeto gdf à lista gdf_list\n",
    "        gdf_list.append(gdf)\n",
    "\n",
    "    # Imprime a lista de objetos gdf lidos\n",
    "    print(gdf_list)\n",
    "    \n",
    "    # Define uma string separadora para uso posterior\n",
    "    separator = \" =====================================================================================================\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33337787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " =====================================================================================================\n",
      "Separando dados do arquivo/mnt/d/ftp1.cptec.inpe.br/pesquisa/das/joao.gerd/EXP18/GSI/dataout/2020010100/diag_amsua_n15_01.2020010100\n",
      " =====================================================================================================\n",
      "Separando dados do arquivo/mnt/d/ftp1.cptec.inpe.br/pesquisa/das/joao.gerd/EXP18/GSI/dataout/2020010106/diag_amsua_n15_01.2020010106\n",
      " =====================================================================================================\n",
      "Separando dados do arquivo/mnt/d/ftp1.cptec.inpe.br/pesquisa/das/joao.gerd/EXP18/GSI/dataout/2020010112/diag_amsua_n15_01.2020010112\n",
      " =====================================================================================================\n",
      "Separando dados do arquivo/mnt/d/ftp1.cptec.inpe.br/pesquisa/das/joao.gerd/EXP18/GSI/dataout/2020010118/diag_amsua_n15_01.2020010118\n",
      " =====================================================================================================\n",
      "Separando dados do arquivo/mnt/d/ftp1.cptec.inpe.br/pesquisa/das/joao.gerd/EXP18/GSI/dataout/2020010200/diag_amsua_n15_01.2020010200\n",
      " =====================================================================================================\n",
      "Separando dados do arquivo/mnt/d/ftp1.cptec.inpe.br/pesquisa/das/joao.gerd/EXP18/GSI/dataout/2020010206/diag_amsua_n15_01.2020010206\n"
     ]
    }
   ],
   "source": [
    "# Inicialização de DataFrame vazio para armazenar dados concatenados\n",
    "df_concatenado2 = pd.DataFrame()\n",
    "\n",
    "# Iteração sobre objetos em gdf_list\n",
    "for objeto in gdf_list:\n",
    "        \n",
    "    print(separator)\n",
    "    print(\"Separando dados do arquivo\" + str(objeto._diagFile))\n",
    "    \n",
    "    # Criação de dicionário de dados a partir do objeto   \n",
    "    dados_dict = {\n",
    "            'lat': objeto.obsInfo[varName].loc[varType].lat,\n",
    "            'lon': objeto.obsInfo[varName].loc[varType].lon,\n",
    "            'elev': objeto.obsInfo[varName].loc[varType].elev,\n",
    "            'nchan': objeto.obsInfo[varName].loc[varType].nchan,\n",
    "            'time': objeto.obsInfo[varName].loc[varType].time,\n",
    "            'iuse': objeto.obsInfo[varName].loc[varType].iuse,\n",
    "            'idqc': objeto.obsInfo[varName].loc[varType].idqc,\n",
    "            'inverr': objeto.obsInfo[varName].loc[varType].inverr,\n",
    "            'oer': objeto.obsInfo[varName].loc[varType].oer,\n",
    "            'obs': objeto.obsInfo[varName].loc[varType].obs,\n",
    "            'omf': objeto.obsInfo[varName].loc[varType].omf,\n",
    "            'omf_nobc': objeto.obsInfo[varName].loc[varType].omf_nobc,\n",
    "            'emiss': objeto.obsInfo[varName].loc[varType].emiss,\n",
    "            'oma': objeto.obsInfo[varName].loc[varType].oma,\n",
    "            'oma_nobc': objeto.obsInfo[varName].loc[varType].oma_nobc,\n",
    "            'imp': objeto.obsInfo[varName].loc[varType].imp,\n",
    "            'dfs': objeto.obsInfo[varName].loc[varType].dfs\n",
    "            }\n",
    "        \n",
    "    # Conversão do dicionário em DataFrame\n",
    "    df_objeto = pd.DataFrame(dados_dict)\n",
    "        \n",
    "    # Concatenação do DataFrame do objeto com o DataFrame concatenado\n",
    "    df_concatenado2 = pd.concat([df_concatenado2, df_objeto], ignore_index=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0fc5b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              lat         lon         elev  nchan      time  iuse  idqc  \\\n",
      "5      -87.504501   19.704599  2735.045410    6.0  0.315833  -1.0  -0.0   \n",
      "6      -87.504501   19.704599  2735.045410    7.0  0.315833   1.0   0.0   \n",
      "7      -87.504501   19.704599  2735.045410    8.0  0.315833   1.0   0.0   \n",
      "8      -87.504501   19.704599  2735.045410    9.0  0.315833   1.0   0.0   \n",
      "9      -87.504501   19.704599  2735.045410   10.0  0.315833   1.0   0.0   \n",
      "...           ...         ...          ...    ...       ...   ...   ...   \n",
      "697642  69.849403  359.350891    -0.074034    8.0  1.566111   1.0   0.0   \n",
      "697643  69.849403  359.350891    -0.074034    9.0  1.566111   1.0   0.0   \n",
      "697644  69.849403  359.350891    -0.074034   10.0  1.566111   1.0   0.0   \n",
      "697646  69.849403  359.350891    -0.074034   12.0  1.566111   1.0   0.0   \n",
      "697647  69.849403  359.350891    -0.074034   13.0  1.566111   1.0   0.0   \n",
      "\n",
      "          inverr       oer         obs       omf  omf_nobc     emiss  \\\n",
      "5       3.557238  0.281441  232.509995  0.047819  1.396376  0.785835   \n",
      "6       3.996653  0.250215  231.679993 -0.077042  0.997890  0.785957   \n",
      "7       3.636355  0.275001  232.139999  0.023001  0.634220  0.786082   \n",
      "8       2.941170  0.340001  234.500000 -0.086228  0.141502  0.786483   \n",
      "9       2.499979  0.400003  237.699997  0.199955  0.774818  0.786483   \n",
      "...          ...       ...         ...       ...       ...       ...   \n",
      "697642  3.636359  0.275000  204.899994 -0.161960 -0.356709  0.861749   \n",
      "697643  2.941168  0.340001  195.630005  0.135727  0.114365  0.863136   \n",
      "697644  2.499974  0.400004  190.899994 -0.168763 -0.033335  0.863136   \n",
      "697646  0.999792  1.000201  199.139999  0.968339  1.759897  0.863136   \n",
      "697647  0.665957  1.501542  218.509995  1.329919  3.487497  0.863136   \n",
      "\n",
      "             oma  oma_nobc       imp       dfs  \n",
      "5       0.002342  1.313053 -0.008105 -0.007727  \n",
      "6      -0.099744  0.969402  0.016040  0.006990  \n",
      "7       0.004765  0.610703 -0.001841 -0.001525  \n",
      "8      -0.097860  0.114162  0.006298  0.002950  \n",
      "9       0.172721  0.729627 -0.025374 -0.013614  \n",
      "...          ...       ...       ...       ...  \n",
      "697642 -0.141681 -0.342003 -0.022391 -0.011943  \n",
      "697643  0.333374  0.324471  0.272694  0.078900  \n",
      "697644 -0.039763  0.091187 -0.067249 -0.054425  \n",
      "697646  0.209626  1.029410 -0.893557 -0.734543  \n",
      "697647  0.047907  2.103161 -1.176385 -1.135482  \n",
      "\n",
      "[317345 rows x 17 columns]\n",
      " =====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Remoção de linhas com valores NaN\n",
    "df_concatenado2.dropna(inplace=True)\n",
    "\n",
    "# Filtrando os canais sub-representados\n",
    "df_concatenado2 = df_concatenado2[~df_concatenado2['nchan'].isin([1, 2, 3, 4, 5, 15])]\n",
    "\n",
    "print(df_concatenado2)\n",
    "\n",
    "print(separator)\n",
    "\n",
    "\n",
    "# Define as classes do target discretizado para uso posterior\n",
    "classes = ['6','7','8','9','10','12','13']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee61150e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             lat         lon       dfs                                \\\n",
      "nchan                              6.0       7.0       8.0       9.0   \n",
      "0     -87.524902  145.937302 -0.027194  0.035069  0.016915  0.007011   \n",
      "1     -87.523804  298.361603  0.031081 -0.039069  0.027618 -0.034395   \n",
      "2     -87.522499   20.437300 -0.014044 -0.163683 -0.183544 -0.051867   \n",
      "3     -87.518898  299.104401 -0.053606  0.041977  0.044107  0.013228   \n",
      "4     -87.518501  216.446106  0.033011  0.056615  0.092010 -0.066644   \n",
      "...          ...         ...       ...       ...       ...       ...   \n",
      "46473  86.502899   93.599602  0.046166  0.003649  0.024471 -0.069496   \n",
      "46479  86.572304  141.941498 -0.123083  0.000894  0.000905 -0.045603   \n",
      "46482  86.586700   80.568298 -0.567365 -0.023826  0.035491 -0.038541   \n",
      "46486  86.592300   78.411201 -0.547038 -0.112739 -0.050711  0.005150   \n",
      "46492  86.700699   98.521400 -0.400516 -0.054911 -0.020432  0.022876   \n",
      "\n",
      "                                          imp  ...       omf            \\\n",
      "nchan      10.0      12.0      13.0       6.0  ...       7.0       8.0   \n",
      "0      0.000284  0.026504  0.038391 -0.013182  ... -0.118112 -0.085846   \n",
      "1     -0.001559  0.000661 -0.062666  0.073114  ...  0.148549 -0.090652   \n",
      "2      0.008408 -0.036524 -0.067257  0.017869  ... -0.278841 -0.351612   \n",
      "3      0.005730  0.001834 -0.012078 -0.082229  ...  0.133205  0.272308   \n",
      "4     -0.015587  0.039170 -0.142622  0.129800  ... -0.116618 -0.197132   \n",
      "...         ...       ...       ...       ...  ...       ...       ...   \n",
      "46473 -0.107215 -0.069744 -0.044981  0.094953  ...  0.039511  0.099254   \n",
      "46479 -0.026066  0.035984 -0.231172 -0.228381  ...  0.021859  0.009455   \n",
      "46482  0.000862 -0.001253 -0.371571 -0.853064  ... -0.046948  0.161720   \n",
      "46486  0.040217 -0.350109 -0.463590 -0.886099  ... -0.180974 -0.149760   \n",
      "46492 -0.018793 -0.035806 -0.220392 -0.618383  ... -0.138513 -0.102403   \n",
      "\n",
      "                                              canal_maior_dfs maior_dfs  \\\n",
      "nchan       9.0      10.0      12.0      13.0                             \n",
      "0     -0.100357  0.085303  0.246246 -0.564692            13.0  0.038391   \n",
      "1      0.107539  0.006364 -0.006260 -0.411204             6.0  0.031081   \n",
      "2     -0.213843  0.113919  0.113521  0.436989            10.0  0.008408   \n",
      "3     -0.103978 -0.090074 -0.274766 -0.200299             8.0  0.044107   \n",
      "4      0.165533  0.057701 -0.409550  0.807013             8.0  0.092010   \n",
      "...         ...       ...       ...       ...             ...       ...   \n",
      "46473 -0.299514 -0.237470 -0.544206  0.137476             6.0  0.046166   \n",
      "46479 -0.182836  0.315167 -0.083398  0.684877            12.0  0.035984   \n",
      "46482 -0.176215 -0.102005 -0.342195 -0.905235             8.0  0.035491   \n",
      "46486 -0.211023 -0.255381  0.665291  1.090713            10.0  0.040217   \n",
      "46492  0.104873  0.254039  0.253330 -0.785645             9.0  0.022876   \n",
      "\n",
      "      canal_maior_imp maior_imp  \n",
      "nchan                            \n",
      "0                 7.0  0.092211  \n",
      "1                 8.0  0.080762  \n",
      "2                12.0  0.030485  \n",
      "3                 7.0  0.108793  \n",
      "4                 8.0  0.243928  \n",
      "...               ...       ...  \n",
      "46473             6.0  0.094953  \n",
      "46479            12.0  0.258178  \n",
      "46482             8.0  0.084228  \n",
      "46486            10.0  0.090353  \n",
      "46492             9.0  0.061930  \n",
      "\n",
      "[38353 rows x 41 columns]\n"
     ]
    }
   ],
   "source": [
    "# Use pivot_table() para pivotar os dados\n",
    "df_pivot2 = df_concatenado2.pivot_table(index=('lat', 'lon'), columns='nchan',\n",
    "                                        values=('obs', 'oma', 'omf','imp', 'dfs'), aggfunc='mean')\n",
    "\n",
    "# Resetando o índice para manter 'linha' como uma coluna\n",
    "df_pivot2.reset_index(inplace=True)\n",
    "\n",
    "# Remoção de linhas com valores NaN\n",
    "df_pivot2.dropna(inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# Lista de colunas que contêm os valores de DFS para cada canal\n",
    "cols_dfs = [('dfs', 6.0), ('dfs', 7.0), ('dfs', 8.0), ('dfs', 9.0), ('dfs', 10.0), ('dfs', 12.0), ('dfs', 13.0)]\n",
    "\n",
    "# Encontrar o canal com o maior valor de DFS em cada linha\n",
    "df_pivot2['canal_maior_dfs'] = df_pivot2[cols_dfs].idxmax(axis=1).str[1]\n",
    "\n",
    "# Encontre o nome da coluna com o maior valor em cada linha\n",
    "max_dfs_column = df_pivot2['dfs'].max(axis=1)\n",
    "\n",
    "# Crie uma nova coluna no DataFrame com a informação do maior dfs em cada linha\n",
    "df_pivot2['maior_dfs'] = max_dfs_column\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Lista de colunas que contêm os valores de imp para cada canal\n",
    "cols_imp = [('imp', 6.0), ('imp', 7.0), ('imp', 8.0), ('imp', 9.0), ('imp', 10.0), ('imp', 12.0), ('imp', 13.0)]\n",
    "\n",
    "# Encontrar o canal com o maior valor de imp em cada linha\n",
    "df_pivot2['canal_maior_imp'] = df_pivot2[cols_imp].idxmax(axis=1).str[1]\n",
    "\n",
    "# Encontre o nome da coluna com o maior valor em cada linha\n",
    "max_dfs_column = df_pivot2['imp'].max(axis=1)\n",
    "\n",
    "# Crie uma nova coluna no DataFrame com a informação do maior dfs em cada linha\n",
    "df_pivot2['maior_imp'] = max_dfs_column\n",
    "\n",
    "\n",
    "# Visualize o DataFrame com a nova coluna\n",
    "print(df_pivot2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc9d16c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiIndex([(            'lat',   ''),\n",
      "            (            'lon',   ''),\n",
      "            (            'dfs',  6.0),\n",
      "            (            'dfs',  7.0),\n",
      "            (            'dfs',  8.0),\n",
      "            (            'dfs',  9.0),\n",
      "            (            'dfs', 10.0),\n",
      "            (            'dfs', 12.0),\n",
      "            (            'dfs', 13.0),\n",
      "            (            'imp',  6.0),\n",
      "            (            'imp',  7.0),\n",
      "            (            'imp',  8.0),\n",
      "            (            'imp',  9.0),\n",
      "            (            'imp', 10.0),\n",
      "            (            'imp', 12.0),\n",
      "            (            'imp', 13.0),\n",
      "            (            'obs',  6.0),\n",
      "            (            'obs',  7.0),\n",
      "            (            'obs',  8.0),\n",
      "            (            'obs',  9.0),\n",
      "            (            'obs', 10.0),\n",
      "            (            'obs', 12.0),\n",
      "            (            'obs', 13.0),\n",
      "            (            'oma',  6.0),\n",
      "            (            'oma',  7.0),\n",
      "            (            'oma',  8.0),\n",
      "            (            'oma',  9.0),\n",
      "            (            'oma', 10.0),\n",
      "            (            'oma', 12.0),\n",
      "            (            'oma', 13.0),\n",
      "            (            'omf',  6.0),\n",
      "            (            'omf',  7.0),\n",
      "            (            'omf',  8.0),\n",
      "            (            'omf',  9.0),\n",
      "            (            'omf', 10.0),\n",
      "            (            'omf', 12.0),\n",
      "            (            'omf', 13.0),\n",
      "            ('canal_maior_dfs',   ''),\n",
      "            (      'maior_dfs',   ''),\n",
      "            ('canal_maior_imp',   ''),\n",
      "            (      'maior_imp',   '')],\n",
      "           names=[None, 'nchan'])\n"
     ]
    }
   ],
   "source": [
    "print(df_pivot2.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8c94ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             lat         lon       dfs                                \\\n",
      "nchan                              6.0       7.0       8.0       9.0   \n",
      "0     -87.524902  145.937302 -0.027194  0.035069  0.016915  0.007011   \n",
      "1     -87.523804  298.361603  0.031081 -0.039069  0.027618 -0.034395   \n",
      "2     -87.522499   20.437300 -0.014044 -0.163683 -0.183544 -0.051867   \n",
      "3     -87.518898  299.104401 -0.053606  0.041977  0.044107  0.013228   \n",
      "4     -87.518501  216.446106  0.033011  0.056615  0.092010 -0.066644   \n",
      "...          ...         ...       ...       ...       ...       ...   \n",
      "46473  86.502899   93.599602  0.046166  0.003649  0.024471 -0.069496   \n",
      "46479  86.572304  141.941498 -0.123083  0.000894  0.000905 -0.045603   \n",
      "46482  86.586700   80.568298 -0.567365 -0.023826  0.035491 -0.038541   \n",
      "46486  86.592300   78.411201 -0.547038 -0.112739 -0.050711  0.005150   \n",
      "46492  86.700699   98.521400 -0.400516 -0.054911 -0.020432  0.022876   \n",
      "\n",
      "                                          imp  ...       omf            \\\n",
      "nchan      10.0      12.0      13.0       6.0  ...       7.0       8.0   \n",
      "0      0.000284  0.026504  0.038391 -0.013182  ... -0.118112 -0.085846   \n",
      "1     -0.001559  0.000661 -0.062666  0.073114  ...  0.148549 -0.090652   \n",
      "2      0.008408 -0.036524 -0.067257  0.017869  ... -0.278841 -0.351612   \n",
      "3      0.005730  0.001834 -0.012078 -0.082229  ...  0.133205  0.272308   \n",
      "4     -0.015587  0.039170 -0.142622  0.129800  ... -0.116618 -0.197132   \n",
      "...         ...       ...       ...       ...  ...       ...       ...   \n",
      "46473 -0.107215 -0.069744 -0.044981  0.094953  ...  0.039511  0.099254   \n",
      "46479 -0.026066  0.035984 -0.231172 -0.228381  ...  0.021859  0.009455   \n",
      "46482  0.000862 -0.001253 -0.371571 -0.853064  ... -0.046948  0.161720   \n",
      "46486  0.040217 -0.350109 -0.463590 -0.886099  ... -0.180974 -0.149760   \n",
      "46492 -0.018793 -0.035806 -0.220392 -0.618383  ... -0.138513 -0.102403   \n",
      "\n",
      "                                              canal_maior_dfs maior_dfs  \\\n",
      "nchan       9.0      10.0      12.0      13.0                             \n",
      "0     -0.100357  0.085303  0.246246 -0.564692            13.0  0.038391   \n",
      "1      0.107539  0.006364 -0.006260 -0.411204             6.0  0.031081   \n",
      "2     -0.213843  0.113919  0.113521  0.436989            10.0  0.008408   \n",
      "3     -0.103978 -0.090074 -0.274766 -0.200299             8.0  0.044107   \n",
      "4      0.165533  0.057701 -0.409550  0.807013             8.0  0.092010   \n",
      "...         ...       ...       ...       ...             ...       ...   \n",
      "46473 -0.299514 -0.237470 -0.544206  0.137476             6.0  0.046166   \n",
      "46479 -0.182836  0.315167 -0.083398  0.684877            12.0  0.035984   \n",
      "46482 -0.176215 -0.102005 -0.342195 -0.905235             8.0  0.035491   \n",
      "46486 -0.211023 -0.255381  0.665291  1.090713            10.0  0.040217   \n",
      "46492  0.104873  0.254039  0.253330 -0.785645             9.0  0.022876   \n",
      "\n",
      "      canal_maior_imp maior_imp  \n",
      "nchan                            \n",
      "0                 7.0  0.092211  \n",
      "1                 8.0  0.080762  \n",
      "2                12.0  0.030485  \n",
      "3                 7.0  0.108793  \n",
      "4                 8.0  0.243928  \n",
      "...               ...       ...  \n",
      "46473             6.0  0.094953  \n",
      "46479            12.0  0.258178  \n",
      "46482             8.0  0.084228  \n",
      "46486            10.0  0.090353  \n",
      "46492             9.0  0.061930  \n",
      "\n",
      "[38353 rows x 41 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_pivot2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d335d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              obs                                                              \\\n",
      "nchan        6.0         7.0         8.0         9.0         10.0        12.0   \n",
      "31164  236.949997  226.679993  216.279999  205.190002  209.259995  226.250000   \n",
      "13252  239.199997  229.089996  219.460007  209.949997  213.910004  235.020004   \n",
      "21232  231.070007  220.320007  209.539993  202.529999  209.080002  228.770004   \n",
      "39188  228.630005  219.460007  211.179993  204.880005  203.800003  213.259995   \n",
      "21136  234.800003  223.550003  211.509995  201.720001  208.529999  228.449997   \n",
      "...           ...         ...         ...         ...         ...         ...   \n",
      "7372   234.559998  228.820007  225.029999  224.570007  228.139999  242.570007   \n",
      "12760  230.759995  222.580002  215.229996  212.839996  218.550003  236.779999   \n",
      "46094  217.940002  212.610001  207.740005  203.380005  204.009995  218.330002   \n",
      "1307   233.100006  230.880005  231.350006  233.179993  236.479996  250.289993   \n",
      "17463  239.690002  228.309998  215.830002  203.309998  209.740005  230.490005   \n",
      "\n",
      "                        oma                      ...                      \\\n",
      "nchan        13.0      6.0       7.0       8.0   ...      10.0      12.0   \n",
      "31164  238.000000 -0.317269 -0.299970 -0.036020  ... -0.021863 -0.362330   \n",
      "13252  247.479996  0.158875  0.050654 -0.201814  ...  0.041738  0.100002   \n",
      "21232  239.470001 -0.106737 -0.225451 -0.044457  ...  0.094806 -0.092537   \n",
      "39188  231.789993 -0.214783 -0.052873 -0.078432  ... -0.427814 -0.354838   \n",
      "21136  240.410004 -0.065744  0.077493  0.083079  ... -0.109320 -0.125487   \n",
      "...           ...       ...       ...       ...  ...       ...       ...   \n",
      "7372   255.000000  0.229863  0.154093 -0.089224  ...  0.310184 -0.060564   \n",
      "12760  247.139999  0.383400  0.172159 -0.211451  ...  0.436398  0.230346   \n",
      "46094  229.789993 -0.354923  0.028141 -0.333878  ... -0.025144  0.432736   \n",
      "1307   263.890015 -0.017530 -0.215400  0.229951  ...  0.385806  0.236604   \n",
      "17463  241.070007  0.172026  0.102915 -0.077607  ...  0.178932 -0.327392   \n",
      "\n",
      "                      omf                                                    \\\n",
      "nchan      13.0      6.0       7.0       8.0       9.0       10.0      12.0   \n",
      "31164  0.203917 -0.249973 -0.273056 -0.085418 -0.178031 -0.032184 -0.330235   \n",
      "13252  0.399332  0.264908  0.255917  0.144041  0.482382  0.461652  0.127527   \n",
      "21232  0.423395 -0.304032 -0.371202  0.040474  0.544944  0.145373 -0.868621   \n",
      "39188  0.761965 -0.266407 -0.220705 -0.392611 -0.285867 -0.716926  0.795167   \n",
      "21136  0.459196  0.076787  0.110308  0.049960 -0.275330 -0.311138 -0.448690   \n",
      "...         ...       ...       ...       ...       ...       ...       ...   \n",
      "7372   0.573749  0.148175  0.190934  0.057660  0.100055  0.531198  0.136556   \n",
      "12760 -0.399235  0.412219  0.033547 -0.558243 -0.205822  0.447114 -0.161658   \n",
      "46094  0.636034 -0.414851  0.050271 -0.326399 -0.277828 -0.549558 -1.295514   \n",
      "1307   0.446551 -0.219502 -0.424448  0.048030 -0.420050  0.351509  0.373692   \n",
      "17463  0.134462  0.307019  0.289768  0.093842  0.096357  0.325722  0.079802   \n",
      "\n",
      "                 \n",
      "nchan      13.0  \n",
      "31164  0.526219  \n",
      "13252  0.948873  \n",
      "21232 -0.125174  \n",
      "39188  3.732618  \n",
      "21136  0.459422  \n",
      "...         ...  \n",
      "7372   0.792487  \n",
      "12760 -1.434033  \n",
      "46094 -1.096390  \n",
      "1307   0.699343  \n",
      "17463  0.746095  \n",
      "\n",
      "[30682 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "# 1. Preparação dos Dados\n",
    "X = df_pivot2[['obs', 'oma', 'omf', 'dfs', 'imp', 'lat', 'lon']]\n",
    "y = df_pivot2[['canal_maior_dfs', 'canal_maior_imp','maior_dfs', 'maior_imp']]\n",
    "\n",
    "# 2. Divisão dos Dados\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train.iloc[:, :21])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eca67c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/patrick/anaconda3/envs/readDiag/lib/python3.7/site-packages/sklearn/utils/validation.py:1692: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['tuple']. An error will be raised in 1.2.\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "# Inicializa e treina o classificador GradientBoostingClassifier com os parâmetros especificados\n",
    "GB = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "    max_depth=3, random_state=0).fit(X_train.iloc[:, :21], y_train.iloc[:, 0])\n",
    "\n",
    "# Calculate predictions on the test set\n",
    "y_pred = GB.predict(X_test.iloc[:, :21])\n",
    "\n",
    "print(y_pred)\n",
    "\n",
    "# Convertendo o array numpy em uma série pandas\n",
    "y_pred_series1 = pd.Series(y_pred)  # Supondo que você queira contar as repetições do primeiro elemento de cada linha\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0bcbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializa e treina o classificador GradientBoostingClassifier com os parâmetros especificados\n",
    "GB2 = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "    max_depth=5, random_state=0).fit(X_train.iloc[:, :21], y_train.iloc[:, 1])\n",
    "\n",
    "# Calculate predictions on the test set\n",
    "y_pred2 = GB2.predict(X_test.iloc[:, :21])\n",
    "\n",
    "print(y_pred2)\n",
    "\n",
    "# Convertendo o array numpy em uma série pandas\n",
    "y_pred_series2 = pd.Series(y_pred2)  # Supondo que você queira contar as repetições do primeiro elemento de cada linha\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebfa905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializa e treina o classificador GradientBoostingClassifier com os parâmetros especificados\n",
    "GB3 = GradientBoostingRegressor(n_estimators=100, learning_rate=1.0,\n",
    "    max_depth=5, random_state=0).fit(X_train.iloc[:, :21], y_train.iloc[:, 2])\n",
    "\n",
    "# Calculate predictions on the test set\n",
    "y_pred3 = GB3.predict(X_test.iloc[:, :21])\n",
    "\n",
    "print(y_pred3)\n",
    "\n",
    "# Convertendo o array numpy em uma série pandas\n",
    "y_pred_series3 = pd.Series(y_pred3)  # Supondo que você queira contar as repetições do primeiro elemento de cada linha\n",
    "#y_pred_series2 = pd.Series(y_pred[:, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10935a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializa e treina o classificador GradientBoostingClassifier com os parâmetros especificados\n",
    "GB4 = GradientBoostingRegressor(n_estimators=100, learning_rate=1.0,\n",
    "    max_depth=5, random_state=0).fit(X_train.iloc[:, :21], y_train.iloc[:, 3])\n",
    "\n",
    "# Calculate predictions on the test set\n",
    "y_pred4 = GB4.predict(X_test.iloc[:, :21])\n",
    "\n",
    "print(y_pred4)\n",
    "\n",
    "# Convertendo o array numpy em uma série pandas\n",
    "y_pred_series4 = pd.Series(y_pred4)  # Supondo que você queira contar as repetições do primeiro elemento de cada linha\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bafe22",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_pivot2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c7574b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contando as repetições e imprimindo a ordem\n",
    "repeticoes1 = y_pred_series1.value_counts()\n",
    "repeticoes2 = y_pred_series2.value_counts()\n",
    "\n",
    "# Contando as repetições e imprimindo a ordem\n",
    "repeticoes3 = y_pred_series3.mean()\n",
    "repeticoes4 = y_pred_series4.mean()\n",
    "\n",
    "\n",
    "\n",
    "# Visualize the channels classified based on the predictions\n",
    "print(\"Channels classified based on predictions for 'dfs':\")\n",
    "print(repeticoes1)\n",
    "\n",
    "print(\"\\nChannels classified based on predictions for 'imp':\")\n",
    "print(repeticoes2)\n",
    "\n",
    "# Visualize the channels classified based on the predictions\n",
    "print(\"Mean based on predictions for 'dfs':\")\n",
    "print(repeticoes3)\n",
    "\n",
    "print(\"\\nMean based on predictions for 'imp':\")\n",
    "print(repeticoes4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5427a296",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MatrizConfusao(YTEST, YPRED, CLASSES):\n",
    "    # Computa a matriz de confusão\n",
    "    matriz_confusao = confusion_matrix(YTEST, YPRED)\n",
    "\n",
    "    # Imprime a matriz de confusão\n",
    "    print(f\"Matriz de Confusão para:\")\n",
    "    print(matriz_confusao)\n",
    "\n",
    "    # Calcula a soma de cada linha da matriz de confusão\n",
    "    sum_by_class = np.sum(matriz_confusao, axis=1)\n",
    "\n",
    "    # Normaliza a matriz de confusão\n",
    "    matriz_confusao_normalized = matriz_confusao / sum_by_class[:, np.newaxis]\n",
    "\n",
    "    # Plota a matriz de confusão normalizada\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(matriz_confusao_normalized, annot=True, fmt=\".3f\", \n",
    "                cmap=\"Blues\", xticklabels=CLASSES, yticklabels=CLASSES)\n",
    "    plt.title(f\"Matriz de Confusão\")\n",
    "    plt.xlabel(\"Valor Previsto\")\n",
    "    plt.ylabel(\"Valor Real\")\n",
    "    plt.savefig(f\"MatrizConfusao.png\", facecolor='white')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Função MatrizConfusao concluída!\")\n",
    "    \n",
    "classes = ['6','7','8','9','10','12','13']\n",
    "MatrizConfusao(y_test.iloc[:, 0], y_pred_series1, classes)\n",
    "MatrizConfusao(y_test.iloc[:, 1], y_pred_series2, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8409c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gera e imprime o relatório de classificação, que inclui métricas como precisão, recall e F1-score\n",
    "relatorio_classificacao = classification_report(y_test.iloc[:, 0], y_pred_series1)\n",
    "print(\"\\nRelatório de Classificação DFS:\")\n",
    "print(relatorio_classificacao)\n",
    "\n",
    "# Gera e imprime o relatório de classificação, que inclui métricas como precisão, recall e F1-score\n",
    "relatorio_classificacao = classification_report(y_test.iloc[:, 1], y_pred_series2)\n",
    "print(\"\\nRelatório de Classificação IMP:\")\n",
    "print(relatorio_classificacao)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b882a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotagem dos dados reais\n",
    "plt.scatter(X_test.iloc[:, 21:28].max(axis=1), y_test.iloc[:, 2], color='blue', label='Dados reais')\n",
    "\n",
    "# Plotagem das previsões\n",
    "plt.scatter(X_test.iloc[:, 21:28].max(axis=1), y_pred3, color='red', linewidth=2, label='Previsões')\n",
    "\n",
    "plt.xlabel('DFS máximo - Dados')\n",
    "plt.ylabel('DFS máximo - Previstos')\n",
    "plt.title('DFS')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plotagem dos dados reais\n",
    "plt.scatter(X_test.iloc[:, 28:35].max(axis=1), y_test.iloc[:, 3], color='blue', label='Dados reais')\n",
    "\n",
    "# Plotagem das previsões\n",
    "plt.scatter(X_test.iloc[:, 28:35].max(axis=1), y_pred4, color='red', linewidth=2, label='Previsões')\n",
    "\n",
    "plt.xlabel('IMP')\n",
    "plt.ylabel('IMP')\n",
    "plt.title('IMP')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27a6cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotagem das previsões\n",
    "plt.scatter(y_pred, y_pred3, color='red', linewidth=2, label='Previsões')\n",
    "\n",
    "# Plotagem dos dados reais\n",
    "plt.scatter(y_test.iloc[:, 0], X_test.iloc[:, 21:28].max(axis=1), color='blue', label='Dados reais')\n",
    "\n",
    "plt.xlabel('CANAL')\n",
    "plt.ylabel('DFS')\n",
    "plt.title('DFS')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Plotagem das previsões\n",
    "plt.scatter(y_pred2, y_pred4, color='red', linewidth=2, label='Previsões')\n",
    "\n",
    "# Plotagem dos dados reais\n",
    "plt.scatter(y_test.iloc[:, 1], X_test.iloc[:, 28:35].max(axis=1), color='blue', label='Dados reais')\n",
    "\n",
    "plt.xlabel('CANAL')\n",
    "plt.ylabel('IMP')\n",
    "plt.title('IMP')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff555279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotagem das previsões\n",
    "plt.scatter(X_test.iloc[:, 21:28].max(axis=1), y_pred3, color='blue', linewidth=2, label='DFS')\n",
    "\n",
    "# Plotagem das previsões\n",
    "#plt.scatter(y_pred[:, 0], y_test.iloc[:, 2], color='red', linewidth=2, label='Previsões - y_pred')\n",
    "\n",
    "plt.xlabel('TESTE')\n",
    "plt.ylabel('PREVISTO')\n",
    "plt.title('DFS')\n",
    "#plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plotagem das previsões\n",
    "plt.scatter(X_test.iloc[:, 28:35].max(axis=1), y_pred4, color='blue', linewidth=2, label='IMP')\n",
    "\n",
    "# Plotagem das previsões\n",
    "#plt.scatter(y_pred[:, 1], y_test.iloc[:, 3], color='red', linewidth=2, label='Previsões - y_pred')\n",
    "\n",
    "plt.xlabel('TESTE')\n",
    "plt.ylabel('PREVISTO')\n",
    "plt.title('IMP')\n",
    "#plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72eb09bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4d033d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "latitude = X_test['lat']\n",
    "longitude = X_test['lon']\n",
    "selecchan =  y_pred\n",
    "\n",
    "# Ajustando a longitude para o intervalo -180 a 180\n",
    "longitude_adj = np.where(longitude > 180, longitude - 360, longitude)\n",
    "\n",
    "# Criando um GeoDataFrame com base nos dados ajustados\n",
    "gdf = gpd.GeoDataFrame(selecchan, geometry=gpd.points_from_xy(longitude_adj, latitude), crs='EPSG:4326')\n",
    "\n",
    "# Plotagem\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)  # Adicionando eixo de cor\n",
    "\n",
    "world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "world.plot(ax=ax, color='lightgrey', edgecolor='black')\n",
    "scatter = gdf.plot(ax=ax, column='y_pred', cmap='viridis', markersize=5, legend=True, cax=cax)\n",
    "scatter.set_xlabel('Longitude')\n",
    "scatter.set_ylabel('Latitude')\n",
    "cax.set_ylabel('Impacto')\n",
    "\n",
    "plt.title('Mapa de Impacto da Previsão do Tempo')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a4e64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "latitude = df_pivot2['lat']\n",
    "longitude = df_pivot2['lon']\n",
    "selecchan =  df_pivot2['canal_maior_imp']\n",
    "\n",
    "# Ajustando a longitude para o intervalo -180 a 180\n",
    "longitude_adj = np.where(longitude > 180, longitude - 360, longitude)\n",
    "\n",
    "# Criando um GeoDataFrame com base nos dados ajustados\n",
    "gdf = gpd.GeoDataFrame(selecchan, geometry=gpd.points_from_xy(longitude_adj, latitude), crs='EPSG:4326')\n",
    "\n",
    "# Plotagem\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)  # Adicionando eixo de cor\n",
    "\n",
    "world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "world.plot(ax=ax, color='lightgrey', edgecolor='black')\n",
    "scatter = gdf.plot(ax=ax, column='canal_maior_imp', cmap='viridis', markersize=5, legend=True, cax=cax)\n",
    "scatter.set_xlabel('Longitude')\n",
    "scatter.set_ylabel('Latitude')\n",
    "cax.set_ylabel('Impacto')\n",
    "\n",
    "plt.title('Mapa de Impacto da Previsão do Tempo')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bbdcaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
