{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1569a849",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3342d0cc",
   "metadata": {},
   "source": [
    "A classificação por floresta aleatória é uma técnica de aprendizado de máquina que combina o poder da aleatoriedade com o poder da média para construir um modelo robusto de classificação. Ela opera criando múltiplas árvores de decisão durante o treinamento e fazendo previsões com base na maioria das previsões das árvores individuais.\n",
    "\n",
    "## Principais Características:\n",
    "- **Árvores de Decisão:** Cada árvore na floresta é construída a partir de uma amostra aleatória do conjunto de dados de treinamento. Isso ajuda a reduzir a correlação entre as árvores individuais, tornando o modelo mais robusto.\n",
    "\n",
    "- **Amostragem Aleatória de Características:** Durante a construção de cada árvore, apenas um subconjunto aleatório das características é considerado para dividir em cada nó da árvore. Isso introduz mais diversidade nas árvores e reduz a probabilidade de overfitting.\n",
    "\n",
    "- **Votação por Maioria:** Ao fazer previsões, cada árvore na floresta contribui com uma votação para determinar a classe final de um exemplo. A classe mais frequente entre todas as árvores é escolhida como a previsão final.\n",
    "\n",
    "## Vantagens:\n",
    "- **Robustez:** Averiguando várias árvores de decisão, o modelo é menos propenso a overfitting e tem uma melhor capacidade de generalização em dados de teste.\n",
    "\n",
    "- **Manuseio de Dados Não Lineares e de Alta Dimensão:** Pode lidar eficazmente com conjuntos de dados com muitas características e interações complexas entre elas.\n",
    "\n",
    "- **Fácil de Usar:** Requer pouca ou nenhuma sintonia de parâmetros e lida bem com dados ausentes.\n",
    "\n",
    "## Limitações:\n",
    "- **Interpretabilidade:** Às vezes, a floresta aleatória pode ser difícil de interpretar em comparação com modelos lineares simples.\n",
    "\n",
    "- **Desempenho em Dados Escaláveis:** Para conjuntos de dados muito grandes, o treinamento de uma floresta aleatória pode se tornar computacionalmente caro.\n",
    "\n",
    "## Aplicações:\n",
    "- **Classificação:** Prever a classe de um exemplo com base em suas características.\n",
    "\n",
    "A implementação da classificação por floresta aleatória no scikit-learn, utilizando RandomForestClassifier, oferece uma interface simples e poderosa para treinar e fazer previsões com este modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448057e0",
   "metadata": {},
   "source": [
    "# Parâmetros do `XGBoost`:\n",
    "\n",
    "**Parâmetros Gerais:**\n",
    "\n",
    "1. `booster`: Define o tipo de modelo a ser usado. Pode ser \"gbtree\" para árvore de decisão, \"gblinear\" para modelo linear ou \"dart\" para adição de dropout à árvore de decisão.\n",
    "\n",
    "2. `verbosity`: Controla o nível de verbosidade ao imprimir mensagens. Pode ser 0 (silencioso), 1 (informações detalhadas) ou 2 (informações detalhadas e mensagens de depuração).\n",
    "\n",
    "3. `nthread`: Número de threads a serem usados para rodar XGBoost. Se definido como -1, o máximo de threads disponíveis será usado.\n",
    "\n",
    "**Parâmetros de Booster (Árvore):**\n",
    "\n",
    "1. `eta` (ou `learning_rate`): Taxa de aprendizado. Controla a contribuição de cada árvore no modelo.\n",
    "\n",
    "2. `gamma`: Mínima redução da perda necessária para fazer uma nova partição em um nó da árvore.\n",
    "\n",
    "3. `max_depth`: Profundidade máxima de cada árvore. Maior profundidade pode levar a overfitting.\n",
    "\n",
    "4. `min_child_weight`: Soma mínima dos pesos das amostras necessária em um nó para continuar dividindo.\n",
    "\n",
    "5. `max_delta_step`: Limite para a atualização do peso de cada árvore. Pode ajudar a tornar o treinamento mais conservador.\n",
    "\n",
    "6. `subsample`: Fração de observações a serem amostradas aleatoriamente para cada árvore. Reduzir pode evitar overfitting.\n",
    "\n",
    "7. `colsample_bytree`: Fração de features a serem amostradas aleatoriamente para cada árvore.\n",
    "\n",
    "8. `lambda` (ou `reg_lambda`): Parâmetro de regularização L2.\n",
    "\n",
    "9. `alpha` (ou `reg_alpha`): Parâmetro de regularização L1.\n",
    "\n",
    "10. `scale_pos_weight`: Controla o balanceamento de classes para classificação desequilibrada.\n",
    "\n",
    "**Parâmetros de Treinamento:**\n",
    "\n",
    "1. `num_boost_round`: Número de iterações de boosting (número de árvores a serem criadas).\n",
    "\n",
    "2. `early_stopping_rounds`: Se a métrica de avaliação não melhorar por esse número de rodadas, o treinamento será interrompido.\n",
    "\n",
    "3. `eval_metric`: A métrica de avaliação a ser usada.\n",
    "\n",
    "**Parâmetros de Avaliação:**\n",
    "\n",
    "1. `eval_set`: Conjunto de dados de validação a serem usados para avaliação durante o treinamento.\n",
    "\n",
    "2. `verbose_eval`: Controla a verbosidade da saída durante a avaliação.\n",
    "\n",
    "**Parâmetros de Regressão Linear (para `gblinear`):**\n",
    "\n",
    "1. `lambda`: Parâmetro de regularização L2.\n",
    "\n",
    "2. `alpha`: Parâmetro de regularização L1.\n",
    "\n",
    "3. `lambda_bias`: Parâmetro de regularização adicional para o termo de viés.\n",
    "\n",
    "**Parâmetros específicos do Dart:**\n",
    "\n",
    "1. `sample_type`: Tipo de amostra a ser usada. Pode ser \"uniform\" para amostras uniformes ou \"weighted\" para amostras ponderadas.\n",
    "\n",
    "2. `normalize_type`: Tipo de normalização a ser usada. Pode ser \"tree\" para normalização por árvore ou \"forest\" para normalização por floresta.\n",
    "\n",
    "3. `rate_drop`: Fração das árvores a serem eliminadas aleatoriamente a cada iteração.\n",
    "\n",
    "4. `skip_drop`: Probabilidade de que uma árvore seja eliminada aleatoriamente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e810b25",
   "metadata": {},
   "source": [
    "Importando bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47f73480",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gsidiag as gd\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afb30182",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import geopandas as gpd\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "from itertools import combinations\n",
    "from itertools import cycle\n",
    "import seaborn as sns\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1357b191",
   "metadata": {},
   "source": [
    "Definindo parâmetros para o uso do readDiag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0806f6b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2020010100', '2020010106', '2020010112', '2020010118', '2020010200', '2020010206']\n"
     ]
    }
   ],
   "source": [
    "#DIRdiag = \"/home/patrick/readDiag/data\"\n",
    "DIRdiag = \"/mnt/d/ftp1.cptec.inpe.br/pesquisa/das/joao.gerd/EXP18/GSI/dataout\"\n",
    "#D:\\ftp1.cptec.inpe.br\\pesquisa\\das\\joao.gerd\\EXP18\\GSI\\dataout\n",
    "varName = \"amsua\"\n",
    "varType = \"n15\"\n",
    "dateIni=\"2020010100\" \n",
    "dateFin=\"2020010206\" \n",
    "nHour = \"6\"          \n",
    "vminOMA = -2.0       \n",
    "vmaxOMA = 2.0        \n",
    "vminSTD = 0.0        \n",
    "vmaxSTD = 14.0       \n",
    "Level = 1000\n",
    "Lay = None           \n",
    "SingleL = \"All\" \n",
    "\n",
    "datei = datetime.strptime(str(dateIni), \"%Y%m%d%H\")\n",
    "datef = datetime.strptime(str(dateFin), \"%Y%m%d%H\")\n",
    "dates = [dates.strftime('%Y%m%d%H') for dates in pd.date_range(datei, datef,freq=\"6H\").tolist()]\n",
    "\n",
    "print(dates)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d45849",
   "metadata": {},
   "source": [
    "Encontrando e listando arquivos para serem usados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c313c705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/mnt/d/ftp1.cptec.inpe.br/pesquisa/das/joao.gerd/EXP18/GSI/dataout/2020010100/diag_amsua_n15_01.2020010100', '/mnt/d/ftp1.cptec.inpe.br/pesquisa/das/joao.gerd/EXP18/GSI/dataout/2020010106/diag_amsua_n15_01.2020010106', '/mnt/d/ftp1.cptec.inpe.br/pesquisa/das/joao.gerd/EXP18/GSI/dataout/2020010112/diag_amsua_n15_01.2020010112', '/mnt/d/ftp1.cptec.inpe.br/pesquisa/das/joao.gerd/EXP18/GSI/dataout/2020010118/diag_amsua_n15_01.2020010118', '/mnt/d/ftp1.cptec.inpe.br/pesquisa/das/joao.gerd/EXP18/GSI/dataout/2020010200/diag_amsua_n15_01.2020010200', '/mnt/d/ftp1.cptec.inpe.br/pesquisa/das/joao.gerd/EXP18/GSI/dataout/2020010206/diag_amsua_n15_01.2020010206']\n",
      "\n",
      "['/mnt/d/ftp1.cptec.inpe.br/pesquisa/das/joao.gerd/EXP18/GSI/dataout/2020010100/diag_amsua_n15_03.2020010100', '/mnt/d/ftp1.cptec.inpe.br/pesquisa/das/joao.gerd/EXP18/GSI/dataout/2020010106/diag_amsua_n15_03.2020010106', '/mnt/d/ftp1.cptec.inpe.br/pesquisa/das/joao.gerd/EXP18/GSI/dataout/2020010112/diag_amsua_n15_03.2020010112', '/mnt/d/ftp1.cptec.inpe.br/pesquisa/das/joao.gerd/EXP18/GSI/dataout/2020010118/diag_amsua_n15_03.2020010118', '/mnt/d/ftp1.cptec.inpe.br/pesquisa/das/joao.gerd/EXP18/GSI/dataout/2020010200/diag_amsua_n15_03.2020010200', '/mnt/d/ftp1.cptec.inpe.br/pesquisa/das/joao.gerd/EXP18/GSI/dataout/2020010206/diag_amsua_n15_03.2020010206']\n"
     ]
    }
   ],
   "source": [
    "paths, pathsc = [], []\n",
    "\n",
    "OuterL = \"01\"        \n",
    "[paths.append(DIRdiag+\"/\"+dt+\"/diag_amsua_n15_\"+OuterL+\".\"+dt) for dt in dates]\n",
    "\n",
    "OuterLc = \"03\"\n",
    "[pathsc.append(DIRdiag+\"/\"+dt+\"/diag_amsua_n15_\"+OuterLc+\".\"+dt) for dt in dates]\n",
    "\n",
    "print(paths)\n",
    "print(\"\")\n",
    "print(pathsc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6771b1b8",
   "metadata": {},
   "source": [
    "Lendo arquivos listado usando o readDiag e concatenando os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a2d6284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Aguarde, o tempo total estimado para a leitura dos arquivos é de 2 minutos e 0 segundos.\n",
      "\n",
      "Reading /mnt/d/ftp1.cptec.inpe.br/pesquisa/das/joao.gerd/EXP18/GSI/dataout/2020010100/diag_amsua_n15_01.2020010100\n",
      " \n",
      ">>> GSI DIAG <<<\n",
      " \n",
      "Reading /mnt/d/ftp1.cptec.inpe.br/pesquisa/das/joao.gerd/EXP18/GSI/dataout/2020010106/diag_amsua_n15_01.2020010106\n",
      " \n",
      ">>> GSI DIAG <<<\n",
      " \n",
      "Reading /mnt/d/ftp1.cptec.inpe.br/pesquisa/das/joao.gerd/EXP18/GSI/dataout/2020010112/diag_amsua_n15_01.2020010112\n",
      " \n",
      ">>> GSI DIAG <<<\n",
      " \n",
      "Reading /mnt/d/ftp1.cptec.inpe.br/pesquisa/das/joao.gerd/EXP18/GSI/dataout/2020010118/diag_amsua_n15_01.2020010118\n",
      " \n",
      ">>> GSI DIAG <<<\n",
      " \n",
      "Reading /mnt/d/ftp1.cptec.inpe.br/pesquisa/das/joao.gerd/EXP18/GSI/dataout/2020010200/diag_amsua_n15_01.2020010200\n",
      " \n",
      ">>> GSI DIAG <<<\n",
      " \n",
      "Reading /mnt/d/ftp1.cptec.inpe.br/pesquisa/das/joao.gerd/EXP18/GSI/dataout/2020010206/diag_amsua_n15_01.2020010206\n",
      " \n",
      ">>> GSI DIAG <<<\n",
      " \n",
      "[<gsidiag.__main__.read_diag object at 0x7f4d4a603c90>, <gsidiag.__main__.read_diag object at 0x7f4d5f154b50>, <gsidiag.__main__.read_diag object at 0x7f4d5efb89d0>, <gsidiag.__main__.read_diag object at 0x7f4d5f026590>, <gsidiag.__main__.read_diag object at 0x7f4d42ebce90>, <gsidiag.__main__.read_diag object at 0x7f4d48d13f90>]\n"
     ]
    }
   ],
   "source": [
    "# Define uma variável booleana 'read' como True para indicar que a leitura dos arquivos será realizada\n",
    "read = True\n",
    "\n",
    "# Verifica se 'read' é True para prosseguir com a leitura dos arquivos\n",
    "if read:        \n",
    "    # Inicializa uma lista vazia para armazenar os objetos gdf\n",
    "    gdf_list = []\n",
    "    # Imprime uma mensagem informando o tempo estimado necessário para a leitura dos arquivos\n",
    "    print(\"\")\n",
    "    print(\"Aguarde, o tempo total estimado para a leitura dos arquivos é de \"+\n",
    "          str(int((float(len(paths))*20 )/60))+\" minutos e \"+\n",
    "          str(int((float(len(paths))*20 )%60))+\" segundos.\")\n",
    "    print(\"\")\n",
    "    # Itera sobre os caminhos dos arquivos e seus caminhos de configuração correspondentes\n",
    "    for path, pathc in zip(paths,pathsc):\n",
    "        # Imprime uma mensagem indicando o arquivo que está sendo lido\n",
    "        print(\"Reading \"+path)\n",
    "        # Lê o arquivo usando a função read_diag do módulo gsidiag e armazena o objeto retornado em gdf\n",
    "        gdf = gd.read_diag(path,pathc)\n",
    "        # Adiciona o objeto gdf à lista gdf_list\n",
    "        gdf_list.append(gdf)\n",
    "\n",
    "    # Imprime a lista de objetos gdf lidos\n",
    "    print(gdf_list)\n",
    "    \n",
    "    # Define uma string separadora para uso posterior\n",
    "    separator = \" =====================================================================================================\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33337787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " =====================================================================================================\n",
      "Separando dados do arquivo/mnt/d/ftp1.cptec.inpe.br/pesquisa/das/joao.gerd/EXP18/GSI/dataout/2020010100/diag_amsua_n15_01.2020010100\n",
      " =====================================================================================================\n",
      "Separando dados do arquivo/mnt/d/ftp1.cptec.inpe.br/pesquisa/das/joao.gerd/EXP18/GSI/dataout/2020010106/diag_amsua_n15_01.2020010106\n",
      " =====================================================================================================\n",
      "Separando dados do arquivo/mnt/d/ftp1.cptec.inpe.br/pesquisa/das/joao.gerd/EXP18/GSI/dataout/2020010112/diag_amsua_n15_01.2020010112\n",
      " =====================================================================================================\n",
      "Separando dados do arquivo/mnt/d/ftp1.cptec.inpe.br/pesquisa/das/joao.gerd/EXP18/GSI/dataout/2020010118/diag_amsua_n15_01.2020010118\n",
      " =====================================================================================================\n",
      "Separando dados do arquivo/mnt/d/ftp1.cptec.inpe.br/pesquisa/das/joao.gerd/EXP18/GSI/dataout/2020010200/diag_amsua_n15_01.2020010200\n",
      " =====================================================================================================\n",
      "Separando dados do arquivo/mnt/d/ftp1.cptec.inpe.br/pesquisa/das/joao.gerd/EXP18/GSI/dataout/2020010206/diag_amsua_n15_01.2020010206\n"
     ]
    }
   ],
   "source": [
    "# Inicialização de DataFrame vazio para armazenar dados concatenados\n",
    "df_concatenado2 = pd.DataFrame()\n",
    "\n",
    "# Iteração sobre objetos em gdf_list\n",
    "for objeto in gdf_list:\n",
    "        \n",
    "    print(separator)\n",
    "    print(\"Separando dados do arquivo\" + str(objeto._diagFile))\n",
    "    \n",
    "    # Criação de dicionário de dados a partir do objeto   \n",
    "    dados_dict = {\n",
    "            'lat': objeto.obsInfo[varName].loc[varType].lat,\n",
    "            'lon': objeto.obsInfo[varName].loc[varType].lon,\n",
    "            'elev': objeto.obsInfo[varName].loc[varType].elev,\n",
    "            'nchan': objeto.obsInfo[varName].loc[varType].nchan,\n",
    "            'time': objeto.obsInfo[varName].loc[varType].time,\n",
    "            'iuse': objeto.obsInfo[varName].loc[varType].iuse,\n",
    "            'idqc': objeto.obsInfo[varName].loc[varType].idqc,\n",
    "            'inverr': objeto.obsInfo[varName].loc[varType].inverr,\n",
    "            'oer': objeto.obsInfo[varName].loc[varType].oer,\n",
    "            'obs': objeto.obsInfo[varName].loc[varType].obs,\n",
    "            'omf': objeto.obsInfo[varName].loc[varType].omf,\n",
    "            'omf_nobc': objeto.obsInfo[varName].loc[varType].omf_nobc,\n",
    "            'emiss': objeto.obsInfo[varName].loc[varType].emiss,\n",
    "            'oma': objeto.obsInfo[varName].loc[varType].oma,\n",
    "            'oma_nobc': objeto.obsInfo[varName].loc[varType].oma_nobc,\n",
    "            'imp': objeto.obsInfo[varName].loc[varType].imp,\n",
    "            'dfs': objeto.obsInfo[varName].loc[varType].dfs\n",
    "            }\n",
    "        \n",
    "    # Conversão do dicionário em DataFrame\n",
    "    df_objeto = pd.DataFrame(dados_dict)\n",
    "        \n",
    "    # Concatenação do DataFrame do objeto com o DataFrame concatenado\n",
    "    df_concatenado2 = pd.concat([df_concatenado2, df_objeto], ignore_index=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0fc5b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              lat         lon         elev  nchan      time  iuse  idqc  \\\n",
      "6      -87.504501   19.704599  2735.045410    7.0  0.315833   1.0   0.0   \n",
      "7      -87.504501   19.704599  2735.045410    8.0  0.315833   1.0   0.0   \n",
      "8      -87.504501   19.704599  2735.045410    9.0  0.315833   1.0   0.0   \n",
      "9      -87.504501   19.704599  2735.045410   10.0  0.315833   1.0   0.0   \n",
      "11     -87.504501   19.704599  2735.045410   12.0  0.315833   1.0   0.0   \n",
      "...           ...         ...          ...    ...       ...   ...   ...   \n",
      "697642  69.849403  359.350891    -0.074034    8.0  1.566111   1.0   0.0   \n",
      "697643  69.849403  359.350891    -0.074034    9.0  1.566111   1.0   0.0   \n",
      "697644  69.849403  359.350891    -0.074034   10.0  1.566111   1.0   0.0   \n",
      "697646  69.849403  359.350891    -0.074034   12.0  1.566111   1.0   0.0   \n",
      "697647  69.849403  359.350891    -0.074034   13.0  1.566111   1.0   0.0   \n",
      "\n",
      "          inverr       oer         obs       omf  omf_nobc     emiss  \\\n",
      "6       3.996653  0.250215  231.679993 -0.077042  0.997890  0.785957   \n",
      "7       3.636355  0.275001  232.139999  0.023001  0.634220  0.786082   \n",
      "8       2.941170  0.340001  234.500000 -0.086228  0.141502  0.786483   \n",
      "9       2.499979  0.400003  237.699997  0.199955  0.774818  0.786483   \n",
      "11      0.999830  1.000170  251.589996 -0.226885  0.981215  0.786483   \n",
      "...          ...       ...         ...       ...       ...       ...   \n",
      "697642  3.636359  0.275000  204.899994 -0.161960 -0.356709  0.861749   \n",
      "697643  2.941168  0.340001  195.630005  0.135727  0.114365  0.863136   \n",
      "697644  2.499974  0.400004  190.899994 -0.168763 -0.033335  0.863136   \n",
      "697646  0.999792  1.000201  199.139999  0.968339  1.759897  0.863136   \n",
      "697647  0.665957  1.501542  218.509995  1.329919  3.487497  0.863136   \n",
      "\n",
      "             oma  oma_nobc       imp       dfs  \n",
      "6      -0.099744  0.969402  0.016040  0.006990  \n",
      "7       0.004765  0.610703 -0.001841 -0.001525  \n",
      "8      -0.097860  0.114162  0.006298  0.002950  \n",
      "9       0.172721  0.729627 -0.025374 -0.013614  \n",
      "11     -0.449353  0.691633  0.150416  0.050466  \n",
      "...          ...       ...       ...       ...  \n",
      "697642 -0.141681 -0.342003 -0.022391 -0.011943  \n",
      "697643  0.333374  0.324471  0.272694  0.078900  \n",
      "697644 -0.039763  0.091187 -0.067249 -0.054425  \n",
      "697646  0.209626  1.029410 -0.893557 -0.734543  \n",
      "697647  0.047907  2.103161 -1.176385 -1.135482  \n",
      "\n",
      "[278908 rows x 17 columns]\n",
      " =====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Remoção de linhas com valores NaN\n",
    "df_concatenado2.dropna(inplace=True)\n",
    "\n",
    "# Filtrando os canais sub-representados\n",
    "df_concatenado2 = df_concatenado2[~df_concatenado2['nchan'].isin([1, 2, 3, 4, 5, 6, 15])]\n",
    "\n",
    "print(df_concatenado2)\n",
    "\n",
    "print(separator)\n",
    "\n",
    "\n",
    "# Define as classes do target discretizado para uso posterior\n",
    "classes = ['6','7','8','9','10','12','13']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee61150e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             lat         lon       dfs                                \\\n",
      "nchan                              7.0       8.0       9.0      10.0   \n",
      "0     -87.524902  145.937302  0.035069  0.016915  0.007011  0.000284   \n",
      "1     -87.523804  298.361603 -0.039069  0.027618 -0.034395 -0.001559   \n",
      "2     -87.522499   20.437300 -0.163683 -0.183544 -0.051867  0.008408   \n",
      "3     -87.518898  299.104401  0.041977  0.044107  0.013228  0.005730   \n",
      "4     -87.518501  216.446106  0.056615  0.092010 -0.066644 -0.015587   \n",
      "...          ...         ...       ...       ...       ...       ...   \n",
      "46505  86.926300  149.013702 -0.010454 -0.049940 -0.043989 -0.086170   \n",
      "46506  86.930099  231.354004  0.038024 -0.158125 -0.212718 -0.294858   \n",
      "46507  86.963997  258.552002 -0.010932  0.003193 -0.000108 -0.000085   \n",
      "46508  87.024101   79.549599  0.007427  0.008859  0.027579  0.031323   \n",
      "46509  87.076797  260.605286 -0.007154 -0.073785  0.001847  0.096013   \n",
      "\n",
      "                                imp            ...       omf            \\\n",
      "nchan      12.0      13.0       7.0       8.0  ...       7.0       8.0   \n",
      "0      0.026504  0.038391  0.092211  0.044508  ... -0.118112 -0.085846   \n",
      "1      0.000661 -0.062666 -0.060837  0.080762  ...  0.148549 -0.090652   \n",
      "2     -0.036524 -0.067257 -0.241147 -0.292153  ... -0.278841 -0.351612   \n",
      "3      0.001834 -0.012078  0.108793  0.095428  ...  0.133205  0.272308   \n",
      "4      0.039170 -0.142622  0.172199  0.243928  ... -0.116618 -0.197132   \n",
      "...         ...       ...       ...       ...  ...       ...       ...   \n",
      "46505 -0.418200 -0.339500 -0.004667 -0.082034  ...  0.041015  0.196039   \n",
      "46506 -0.012131 -0.269775  0.085371 -0.145604  ... -0.196902  0.200733   \n",
      "46507 -0.162681 -1.297960  0.009583  0.008635  ...  0.030823 -0.035302   \n",
      "46508 -0.274902 -0.254240  0.022426  0.037360  ...  0.042678  0.033149   \n",
      "46509 -0.093524 -0.022253 -0.011614 -0.115995  ... -0.068926 -0.217753   \n",
      "\n",
      "                                              canal_maior_dfs maior_dfs  \\\n",
      "nchan       9.0      10.0      12.0      13.0                             \n",
      "0     -0.100357  0.085303  0.246246 -0.564692            13.0  0.038391   \n",
      "1      0.107539  0.006364 -0.006260 -0.411204             8.0  0.027618   \n",
      "2     -0.213843  0.113919  0.113521  0.436989            10.0  0.008408   \n",
      "3     -0.103978 -0.090074 -0.274766 -0.200299             8.0  0.044107   \n",
      "4      0.165533  0.057701 -0.409550  0.807013             8.0  0.092010   \n",
      "...         ...       ...       ...       ...             ...       ...   \n",
      "46505  0.132004  0.153620  0.810795  0.811338             7.0 -0.010454   \n",
      "46506  0.239797  0.543721  0.783198 -0.744947             7.0  0.038024   \n",
      "46507 -0.003343  0.032417 -0.328125 -1.709236             8.0  0.003193   \n",
      "46508  0.113541  0.055736 -0.944505  1.178570            10.0  0.031323   \n",
      "46509  0.004984  0.386703  0.592573  0.150777            10.0  0.096013   \n",
      "\n",
      "      canal_maior_imp maior_imp  \n",
      "nchan                            \n",
      "0                 7.0  0.092211  \n",
      "1                 8.0  0.080762  \n",
      "2                12.0  0.030485  \n",
      "3                 7.0  0.108793  \n",
      "4                 8.0  0.243928  \n",
      "...               ...       ...  \n",
      "46505             7.0 -0.004667  \n",
      "46506             7.0  0.085371  \n",
      "46507             7.0  0.009583  \n",
      "46508            10.0  0.188985  \n",
      "46509            10.0  0.216685  \n",
      "\n",
      "[46359 rows x 36 columns]\n"
     ]
    }
   ],
   "source": [
    "# Use pivot_table() para pivotar os dados\n",
    "df_pivot2 = df_concatenado2.pivot_table(index=('lat', 'lon'), columns='nchan',\n",
    "                                        values=('obs', 'oma', 'omf','imp', 'dfs'), aggfunc='mean')\n",
    "\n",
    "# Resetando o índice para manter 'linha' como uma coluna\n",
    "df_pivot2.reset_index(inplace=True)\n",
    "\n",
    "# Remoção de linhas com valores NaN\n",
    "df_pivot2.dropna(inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# Lista de colunas que contêm os valores de DFS para cada canal\n",
    "cols_dfs = [('dfs', 7.0), ('dfs', 8.0), ('dfs', 9.0), ('dfs', 10.0), ('dfs', 12.0), ('dfs', 13.0)]\n",
    "\n",
    "# Encontrar o canal com o maior valor de DFS em cada linha\n",
    "df_pivot2['canal_maior_dfs'] = df_pivot2[cols_dfs].idxmax(axis=1).str[1]\n",
    "\n",
    "# Encontre o nome da coluna com o maior valor em cada linha\n",
    "max_dfs_column = df_pivot2['dfs'].max(axis=1)\n",
    "\n",
    "# Crie uma nova coluna no DataFrame com a informação do maior dfs em cada linha\n",
    "df_pivot2['maior_dfs'] = max_dfs_column\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Lista de colunas que contêm os valores de imp para cada canal\n",
    "cols_imp = [('imp', 7.0), ('imp', 8.0), ('imp', 9.0), ('imp', 10.0), ('imp', 12.0), ('imp', 13.0)]\n",
    "\n",
    "# Encontrar o canal com o maior valor de imp em cada linha\n",
    "df_pivot2['canal_maior_imp'] = df_pivot2[cols_imp].idxmax(axis=1).str[1]\n",
    "\n",
    "# Encontre o nome da coluna com o maior valor em cada linha\n",
    "max_dfs_column = df_pivot2['imp'].max(axis=1)\n",
    "\n",
    "# Crie uma nova coluna no DataFrame com a informação do maior dfs em cada linha\n",
    "df_pivot2['maior_imp'] = max_dfs_column\n",
    "\n",
    "\n",
    "# Visualize o DataFrame com a nova coluna\n",
    "print(df_pivot2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc9d16c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiIndex([(            'lat',   ''),\n",
      "            (            'lon',   ''),\n",
      "            (            'dfs',  7.0),\n",
      "            (            'dfs',  8.0),\n",
      "            (            'dfs',  9.0),\n",
      "            (            'dfs', 10.0),\n",
      "            (            'dfs', 12.0),\n",
      "            (            'dfs', 13.0),\n",
      "            (            'imp',  7.0),\n",
      "            (            'imp',  8.0),\n",
      "            (            'imp',  9.0),\n",
      "            (            'imp', 10.0),\n",
      "            (            'imp', 12.0),\n",
      "            (            'imp', 13.0),\n",
      "            (            'obs',  7.0),\n",
      "            (            'obs',  8.0),\n",
      "            (            'obs',  9.0),\n",
      "            (            'obs', 10.0),\n",
      "            (            'obs', 12.0),\n",
      "            (            'obs', 13.0),\n",
      "            (            'oma',  7.0),\n",
      "            (            'oma',  8.0),\n",
      "            (            'oma',  9.0),\n",
      "            (            'oma', 10.0),\n",
      "            (            'oma', 12.0),\n",
      "            (            'oma', 13.0),\n",
      "            (            'omf',  7.0),\n",
      "            (            'omf',  8.0),\n",
      "            (            'omf',  9.0),\n",
      "            (            'omf', 10.0),\n",
      "            (            'omf', 12.0),\n",
      "            (            'omf', 13.0),\n",
      "            ('canal_maior_dfs',   ''),\n",
      "            (      'maior_dfs',   ''),\n",
      "            ('canal_maior_imp',   ''),\n",
      "            (      'maior_imp',   '')],\n",
      "           names=[None, 'nchan'])\n"
     ]
    }
   ],
   "source": [
    "print(df_pivot2.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d335d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              obs                                                              \\\n",
      "nchan        7.0         8.0         9.0         10.0        12.0        13.0   \n",
      "38105  218.809998  212.399994  209.350006  213.199997  235.429993  247.740005   \n",
      "40949  215.880005  210.250000  203.570007  202.630005  222.610001  245.380005   \n",
      "5155   230.610001  228.820007  229.190002  232.220001  245.539993  256.540009   \n",
      "21886  221.000000  209.660004  201.580002  208.690002  230.270004  239.389999   \n",
      "6557   228.199997  226.320007  226.770004  229.130005  246.160004  258.559998   \n",
      "...           ...         ...         ...         ...         ...         ...   \n",
      "11305  222.220001  216.949997  217.119995  223.139999  242.020004  254.619995   \n",
      "44881  214.210007  210.009995  205.979996  208.179993  230.160004  245.399994   \n",
      "38268  229.169998  225.910004  224.789993  226.699997  234.979996  239.580002   \n",
      "860    228.320007  228.770004  232.240005  236.039993  247.990005  260.730011   \n",
      "15829  227.169998  215.800003  205.490005  210.979996  230.610001  242.699997   \n",
      "\n",
      "            oma                                ...                 omf  \\\n",
      "nchan      7.0       8.0       9.0       10.0  ...      13.0      7.0    \n",
      "38105 -0.056860  0.023571  0.050166  0.169003  ... -0.752659 -0.065296   \n",
      "40949 -0.143819 -0.146323 -0.022160  0.015848  ... -0.073695 -0.218416   \n",
      "5155  -0.047875 -0.001534 -0.144373  0.174426  ... -0.387642 -0.063922   \n",
      "21886 -0.077142  0.191631  0.206615 -0.376083  ...  0.463094  0.045774   \n",
      "6557   0.139423  0.076832  0.288396 -0.335834  ... -0.546795  0.360706   \n",
      "...         ...       ...       ...       ...  ...       ...       ...   \n",
      "11305  0.054428  0.076834 -0.021686  0.554023  ...  0.917777  0.083964   \n",
      "44881 -0.068879  0.023170 -0.009231  0.450882  ...  0.297077 -0.037762   \n",
      "38268 -0.064110 -0.209679 -0.217384 -0.205911  ... -0.277031 -0.251527   \n",
      "860    0.197301  0.039000 -0.133971  0.187199  ... -0.200583  0.226655   \n",
      "15829  0.002562 -0.068884 -0.185683 -0.090976  ...  0.287634  0.085057   \n",
      "\n",
      "                                                              dfs            \\\n",
      "nchan      8.0       9.0       10.0      12.0      13.0      7.0       8.0    \n",
      "38105 -0.024834 -0.127224 -0.039663  0.119643  0.004069 -0.002203 -0.004371   \n",
      "40949 -0.301792 -0.250317 -0.412606 -2.612558 -2.763132 -0.065147 -0.170615   \n",
      "5155   0.002558 -0.045066  0.327102  0.368564  0.361331 -0.004102 -0.000038   \n",
      "21886  0.310385  0.168905 -0.261700  0.694278  0.599334 -0.022506 -0.134033   \n",
      "6557   0.315678  0.422628 -0.117292  0.472058  0.177083 -0.319273 -0.274175   \n",
      "...         ...       ...       ...       ...       ...       ...       ...   \n",
      "11305  0.139746  0.175654  0.670673  0.739729  1.477666 -0.009920 -0.031970   \n",
      "44881  0.139002 -0.012141  0.205396  0.607407  0.978536  0.004699 -0.058549   \n",
      "38268 -0.624199 -0.583420 -0.451472 -0.376347 -0.185843 -0.188558 -0.940881   \n",
      "860   -0.062887 -0.240259  0.139487 -0.879958 -0.329523 -0.025686 -0.023288   \n",
      "15829  0.044859  0.127455  0.084543  0.068885 -0.090735 -0.028064 -0.018554   \n",
      "\n",
      "                 \n",
      "nchan      9.0   \n",
      "38105 -0.066377  \n",
      "40949 -0.167975  \n",
      "5155   0.013163  \n",
      "21886  0.018734  \n",
      "6557  -0.166854  \n",
      "...         ...  \n",
      "11305 -0.101952  \n",
      "44881 -0.000104  \n",
      "38268 -0.628094  \n",
      "860   -0.075107  \n",
      "15829 -0.117386  \n",
      "\n",
      "[37087 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "# 1. Preparação dos Dados\n",
    "X = df_pivot2[['obs', 'oma', 'omf', 'dfs', 'imp', 'lat', 'lon']]\n",
    "y = df_pivot2[['canal_maior_dfs', 'canal_maior_imp','maior_dfs', 'maior_imp']]\n",
    "\n",
    "# 2. Divisão dos Dados\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train.iloc[:, :21])\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "03d12a98",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "#qid = df_pivot2['lat'].astype(str) + '_' + df_pivot2['lon'].astype(str)\n",
    "\n",
    "qid = df_concatenado2['nchan']\n",
    "\n",
    "# Create a dictionary to map unique query IDs to numeric identifiers\n",
    "#qid_to_numeric = {qid_val: i for i, qid_val in enumerate(qid.unique())}\n",
    "\n",
    "# Encode the query IDs numerically\n",
    "#qid_numeric = qid.map(qid_to_numeric)\n",
    "\n",
    "# Sort the numeric query IDs\n",
    "#qid_numeric_sorted = qid_numeric.values[qid_sorted_idx]\n",
    "\n",
    "#X = df_concatenado2[['obs', 'oma', 'omf']]\n",
    "\n",
    "# Separa as features (dfs)\n",
    "X = df_concatenado2[['obs', 'oma', 'omf', 'lat', 'lon']]  # Features (dfs)\n",
    "\n",
    "# Obtém a lista de targets\n",
    "targets = df_concatenado2[['imp']]\n",
    "\n",
    "# Inicializa uma lista para armazenar os classificadores treinados\n",
    "classificadores = []\n",
    "\n",
    "# Loop sobre cada target\n",
    "for target in targets:\n",
    "    # Separa o target atual\n",
    "    y = df_concatenado2[target]\n",
    "    \n",
    "    # Inicializa o classificador XGBoost para ranking\n",
    "    ranker = xgb.XGBRanker(tree_method=\"hist\", \n",
    "                           lambdarank_num_pair_per_sample=8, \n",
    "                           objective=\"rank:ndcg\", \n",
    "                           lambdarank_pair_method=\"topk\")\n",
    "    \n",
    "    # Ajusta o classificador XGBoost aos dados de entrada\n",
    "#    ranker.fit(X, y, qid=qid_numeric_sorted)\n",
    "    ranker.fit(X, y, qid=qid)\n",
    "               \n",
    "    # Armazena o classificador treinado\n",
    "    classificadores.append(ranker)\n",
    "\n",
    "# Realiza previsões do classificador XGBoost nos dados de entrada\n",
    "scores = np.zeros((X.shape[0], len(targets)))\n",
    "\n",
    "# Loop sobre cada classificador\n",
    "for i, ranker in enumerate(classificadores):\n",
    "    # Faz as previsões para o target atual\n",
    "    scores[:, i] = ranker.predict(X)\n",
    "\n",
    "print(scores)\n",
    "    \n",
    "# Soma as previsões dos diferentes targets para obter uma pontuação global\n",
    "scores_sum = np.sum(scores, axis=1)\n",
    "print(scores_sum)\n",
    "\n",
    "# Ordena os índices das previsões do classificador em ordem decrescente\n",
    "sorted_idx = np.argsort(scores_sum)[::-1]\n",
    "\n",
    "# Ordena os escores de relevância do mais relevante para o menos relevante\n",
    "scores_sorted = scores_sum[sorted_idx]\n",
    "print(scores_sorted)\n",
    "\n",
    "# Obtém os canais de radiância com maior impacto para cada ponto de grade\n",
    "#canal_maior_imp = df_pivot2[\"canal_maior_dfs\"].iloc[sorted_idx]\n",
    "#maior_imp = df_pivot2[\"maior_dfs\"].iloc[sorted_idx]\n",
    "\n",
    "# Imprime os resultados\n",
    "#resultados = pd.DataFrame({\"canal_maior_dfs\": canal_maior_imp, \"maior_dfs\": maior_imp, \"scores\": scores_sorted})\n",
    "#print(resultados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf9b0e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-map:0.99657\tvalidation_1-map:0.99663\n",
      "[1]\tvalidation_0-map:0.99937\tvalidation_1-map:0.99944\n",
      "[2]\tvalidation_0-map:0.99912\tvalidation_1-map:0.99924\n",
      "[3]\tvalidation_0-map:0.99872\tvalidation_1-map:0.99885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/patrick/anaconda3/envs/readDiag/lib/python3.7/site-packages/xgboost/sklearn.py:797: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4]\tvalidation_0-map:0.99927\tvalidation_1-map:0.99937\n",
      "[5]\tvalidation_0-map:0.99900\tvalidation_1-map:0.99910\n",
      "[6]\tvalidation_0-map:0.99901\tvalidation_1-map:0.99911\n",
      "[7]\tvalidation_0-map:0.99905\tvalidation_1-map:0.99916\n",
      "[8]\tvalidation_0-map:0.99915\tvalidation_1-map:0.99925\n",
      "[9]\tvalidation_0-map:0.99904\tvalidation_1-map:0.99913\n",
      "[10]\tvalidation_0-map:0.99911\tvalidation_1-map:0.99919\n",
      "              lat         lon         obs       oma       omf\n",
      "75801   68.613701  283.049194  221.429993  0.243796  0.326914\n",
      "291666  24.273899  243.814804  227.020004  0.009374  0.310378\n",
      "96756  -47.671600  332.006012  225.770004 -0.137224 -0.113129\n",
      "144021  28.127100   65.620697  224.559998  0.154185 -0.117707\n",
      "112281 -28.156401  337.455505  224.139999  0.061626  0.108615\n",
      "...           ...         ...         ...       ...       ...\n",
      "630147 -45.145599  163.353394  252.520004  0.578940  1.242548\n",
      "210522  78.977203   12.440600  213.740005 -0.112373 -0.553824\n",
      "352392  -4.372500   21.472200  238.050003  0.844087  1.650331\n",
      "54147   60.668701  148.373795  244.350006  0.075834  0.969443\n",
      "17517   23.009001   81.628304  239.149994 -0.895193  0.199651\n",
      "\n",
      "[55782 rows x 5 columns]\n",
      "[[2]\n",
      " [2]\n",
      " [3]\n",
      " ...\n",
      " [1]\n",
      " [2]\n",
      " [3]]\n",
      "[0.25333685 0.24957089 0.7198354  ... 0.14570343 0.3544932  0.7885273 ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "# Ordenando o DataFrame com base na coluna 'nchan'\n",
    "df_concatenado2.sort_values(by='nchan', inplace=True)\n",
    "\n",
    "# Convertendo 'nchan' para string para ser usado como qid\n",
    "qid = df_concatenado2['nchan']\n",
    "\n",
    "# Separando as features (dfs)\n",
    "X = df_concatenado2[['lat', 'lon', 'obs', 'oma', 'omf']]  # Features (dfs)\n",
    "y = df_concatenado2[['imp']]  # Target\n",
    "\n",
    "thresholds = [-2, -1, 0, 1, 2]  # Defina os limites conforme necessário\n",
    "y_classes = np.digitize(y, thresholds, right=True)\n",
    "\n",
    "# Dividindo os dados em conjuntos de treinamento e teste\n",
    "X_train, X_test, y_train, y_test, qid_train, qid_test = train_test_split(\n",
    "    X, y_classes, qid, train_size=0.8, shuffle=True, random_state=42\n",
    ")\n",
    "\n",
    "# Ordenando os conjuntos de treinamento pelo qid\n",
    "train_sorted_idx = np.argsort(qid_train)\n",
    "X_train = X_train.iloc[train_sorted_idx]\n",
    "y_train = y_train[train_sorted_idx]\n",
    "qid_train = qid_train.iloc[train_sorted_idx]\n",
    "\n",
    "# Ordenando os conjuntos de teste pelo qid\n",
    "test_sorted_idx = np.argsort(qid_test)\n",
    "X_test = X_test.iloc[test_sorted_idx]\n",
    "y_test = y_test[test_sorted_idx]\n",
    "qid_test = qid_test.iloc[test_sorted_idx]\n",
    "\n",
    "# Convertendo qid para inteiros\n",
    "qid_train = qid_train.astype(int)\n",
    "qid_test = qid_test.astype(int)\n",
    "\n",
    "# Configurando os parâmetros do XGBoost Ranker\n",
    "params = {\n",
    "    'objective': 'rank:pairwise',  # Usando a função de perda NDCG para problemas de ranking\n",
    "    'eval_metric': 'map',      # Métrica de avaliação durante o treinamento\n",
    "    'tree_method': 'hist',      # Método de construção da árvore para treinamento mais rápido\n",
    "    'learning_rate': 0.2,       # Taxa de aprendizado (ajuste conforme necessário)\n",
    "    'max_depth': 6,             # Profundidade máxima das árvores\n",
    "    'min_child_weight': 3,      # Peso mínimo da instância necessária em cada nó folha\n",
    "    'subsample': 0.8,           # Proporção de instâncias de treinamento a serem amostradas para treinar cada árvore\n",
    "    'colsample_bytree': 0.8,    # Proporção de características a serem amostradas para treinar cada árvore\n",
    "    'gamma': 0.1,               # Parâmetro de regularização para evitar overfitting\n",
    "    'n_estimators': 200         # Número de árvores (estimadores) no modelo\n",
    "}\n",
    "\n",
    "# Criando o objeto XGBoost Ranker\n",
    "ranker = xgb.XGBRanker(**params)\n",
    "\n",
    "# Treinando o modelo\n",
    "ranker.fit(X_train, y_train, qid=qid_train.values,\n",
    "            eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "            eval_group=[qid_train.value_counts().sort_index().values,\n",
    "                        qid_test.value_counts().sort_index().values],\n",
    "            early_stopping_rounds=10, verbose=True)\n",
    "\n",
    "# Realizando previsões no conjunto de teste\n",
    "preds = ranker.predict(X_test)\n",
    "\n",
    "print(X_test)\n",
    "print(y_test)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72a11e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     previsoes\n",
      "qid           \n",
      "7     0.472130\n",
      "8     0.453402\n",
      "9     0.444778\n",
      "10    0.456390\n",
      "12    0.439923\n",
      "13    0.455529\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Criar um dataframe com as previsões, qid e os dados originais\n",
    "resultados = pd.DataFrame({'qid': qid_test, 'previsoes': preds})\n",
    "\n",
    "# Agrupar por qid e calcular a média das previsões para cada canal\n",
    "media_por_canal = resultados.groupby('qid').mean()\n",
    "\n",
    "print(media_por_canal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b738198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12    9410\n",
      "9     9334\n",
      "13    9315\n",
      "10    9268\n",
      "7     9257\n",
      "8     9198\n",
      "Name: qid, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(resultados['qid'].value_counts())"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ad094e49",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "\n",
    "df_concatenado2.sort_values(by='nchan', inplace=True)\n",
    "\n",
    "qid = df_concatenado2['nchan'].astype(str)\n",
    "\n",
    "# Separa as features (dfs)\n",
    "X = df_concatenado2[['lat', 'lon', 'obs', 'oma', 'omf', 'dfs']]  # Features (dfs)\n",
    "\n",
    "y = df_concatenado2[['imp']]\n",
    "    \n",
    "# Inicializa o classificador XGBoost para ranking\n",
    "#ranker = xgb.XGBRanker(tree_method=\"hist\", \n",
    "#                        lambdarank_num_pair_per_sample=7, \n",
    "#                        objective=\"rank:ndcg\", \n",
    "#                        lambdarank_pair_method=\"topk\")\n",
    "\n",
    "ranker = xgb.XGBRanker(tree_method=\"hist\", \n",
    "                        objective=\"rank:ndcg\", \n",
    "                        eval_metric=\"ndcg\")\n",
    "\n",
    "ranker.fit(X, y, qid=qid)\n",
    "\n",
    "scores = ranker.predict(X)\n",
    "\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "598420f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              lat         lon         obs       oma       omf\n",
      "6      -87.504501   19.704599  231.679993 -0.099744 -0.077042\n",
      "409431 -12.357200  190.132294  227.610001  0.119951  0.202303\n",
      "115326 -50.188999   23.128300  226.759995  0.173446 -0.089976\n",
      "409416 -12.182500  188.734207  228.860001  0.186428  0.289750\n",
      "409401 -12.485000  187.313004  228.949997 -0.009565  0.095424\n",
      "...           ...         ...         ...       ...       ...\n",
      "468807 -10.036100  336.723206  238.100006 -0.855764  0.025855\n",
      "91557   54.083599  311.948914  207.449997  0.145808  0.236507\n",
      "468792 -11.142900  359.108307  238.619995 -0.222396  1.023522\n",
      "468957  -8.250300  338.404114  239.470001 -0.007398  0.345495\n",
      "697647  69.849403  359.350891  218.509995  0.047907  1.329919\n",
      "\n",
      "[278908 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c4f2e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             imp\n",
      "6       0.016040\n",
      "409431 -0.106147\n",
      "115326  0.087946\n",
      "409416 -0.196777\n",
      "409401 -0.036052\n",
      "...          ...\n",
      "468807  0.487408\n",
      "91557  -0.023100\n",
      "468792 -0.664931\n",
      "468957 -0.079472\n",
      "697647 -1.176385\n",
      "\n",
      "[278908 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f15ee55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6          7.0\n",
      "409431     7.0\n",
      "115326     7.0\n",
      "409416     7.0\n",
      "409401     7.0\n",
      "          ... \n",
      "468807    13.0\n",
      "91557     13.0\n",
      "468792    13.0\n",
      "468957    13.0\n",
      "697647    13.0\n",
      "Name: nchan, Length: 278908, dtype: float32\n"
     ]
    }
   ],
   "source": [
    "print(qid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca63651",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
