{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47f73480",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gsidiag as gd\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afb30182",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0806f6b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2020010100', '2020010106']\n"
     ]
    }
   ],
   "source": [
    "DIRdiag = \"/home/patrick/readDiag/data\"\n",
    "\n",
    "varName = \"amsua\"\n",
    "varType = \"n15\"\n",
    "dateIni=\"2020010100\" \n",
    "dateFin=\"2020010106\" \n",
    "nHour = \"6\"          \n",
    "vminOMA = -2.0       \n",
    "vmaxOMA = 2.0        \n",
    "vminSTD = 0.0        \n",
    "vmaxSTD = 14.0       \n",
    "Level = 1000\n",
    "Lay = None           \n",
    "SingleL = \"All\" \n",
    "\n",
    "datei = datetime.strptime(str(dateIni), \"%Y%m%d%H\")\n",
    "datef = datetime.strptime(str(dateFin), \"%Y%m%d%H\")\n",
    "dates = [dates.strftime('%Y%m%d%H') for dates in pd.date_range(datei, datef,freq=\"6H\").tolist()]\n",
    "\n",
    "print(dates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c313c705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/patrick/readDiag/data/2020010100/diag_amsua_n15_01.2020010100', '/home/patrick/readDiag/data/2020010106/diag_amsua_n15_01.2020010106']\n",
      "\n",
      "['/home/patrick/readDiag/data/2020010100/diag_amsua_n15_03.2020010100', '/home/patrick/readDiag/data/2020010106/diag_amsua_n15_03.2020010106']\n"
     ]
    }
   ],
   "source": [
    "paths, pathsc = [], []\n",
    "\n",
    "OuterL = \"01\"        \n",
    "[paths.append(DIRdiag+\"/\"+dt+\"/diag_amsua_n15_\"+OuterL+\".\"+dt) for dt in dates]\n",
    "\n",
    "OuterLc = \"03\"\n",
    "[pathsc.append(DIRdiag+\"/\"+dt+\"/diag_amsua_n15_\"+OuterLc+\".\"+dt) for dt in dates]\n",
    "\n",
    "print(paths)\n",
    "print(\"\")\n",
    "print(pathsc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9981ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Aguarde, o tempo total estimado para a leitura dos arquivos é de 0 minutos e 40 segundos.\n",
      "\n",
      "Reading /home/patrick/readDiag/data/2020010100/diag_amsua_n15_01.2020010100\n",
      " \n",
      ">>> GSI DIAG <<<\n",
      " \n",
      "Reading /home/patrick/readDiag/data/2020010106/diag_amsua_n15_01.2020010106\n",
      " \n",
      ">>> GSI DIAG <<<\n",
      " \n",
      "[<gsidiag.__main__.read_diag object at 0x7f1230bd3c10>, <gsidiag.__main__.read_diag object at 0x7f123810c850>]\n",
      " =====================================================================================================\n",
      "Separando dados do arquivo/home/patrick/readDiag/data/2020010100/diag_amsua_n15_01.2020010100\n",
      " =====================================================================================================\n",
      " =====================================================================================================\n",
      "Separando dados do arquivo/home/patrick/readDiag/data/2020010106/diag_amsua_n15_01.2020010106\n",
      " =====================================================================================================\n",
      "Done!\n",
      "{'ch7imp': [0.016040072, 0.035045207, -0.10521981, -0.009949882, 0.045968544, -0.19128995, 0.031039009, -0.08090803, 0.004787808, 0.054777786, 0.05594914, -0.09248838, -0.2304151, -0.06043214, -0.07786249, 0.102988675, -0.051420785, -0.16134001, -0.12109892, 0.17194211], 'ch8imp': [-0.0018412294, -0.004188078, -0.011863028, -0.11141313, 0.005730015, -0.00782152, 0.017537262, 0.029750472, 0.015272359, 0.06732387, -0.070308276, -0.06254356, -0.006029169, -0.02355751, -0.020734858, -0.057218097, -0.09351446, -0.23004721, -0.17218839, -0.07476137], 'ch9imp': [0.006297947, 0.0070296945, 0.000659538, 0.0037943546, -0.08121186, -0.0012931841, -0.08819933, -0.07827712, 0.0033368345, -0.1318514, -0.03655697, 0.013799096, 0.039470524, 0.060695104, 0.0009874884, 0.022474341, 0.012282538, 0.05491115, 0.008176253, 0.009510833], 'ch10imp': [-0.02537354, 0.08900937, 0.0073288367, 0.016004957, -0.021229394, -0.0006614767, 0.02321331, 0.017354004, 0.036492243, -0.0043941014, -0.026544789, -0.036672242, 0.0022285008, -0.0121564325, -0.0042955936, 0.00015041989, 0.016626004, 0.015826318, -0.0061657396, -0.033838317], 'ch12imp': [0.15041573, 0.17378081, 0.27834877, 0.04092068, 0.106664, 0.08569123, 0.0061156787, 0.0009041546, -0.010649247, -0.0035750659, -0.04355138, 0.043587197, -0.015308762, 0.00013822249, 0.00049845304, -0.01789072, -0.032512497, -0.045840368, -0.08873785, -0.0729913], 'ch13imp': [-0.026127167, -0.12507783, 0.0018596704, -0.028622517, 0.1461525, 0.0293557, 0.16980064, -0.109657966, 0.078096375, -0.04605193, -0.09251071, -0.14208724, 0.12437626, -0.06745439, -0.20242172, -0.48732308, -0.12790824, -0.23069529, 0.19838585, 0.14997242]}\n",
      "         ch7       ch8       ch9      ch10      ch12      ch13\n",
      "0   0.016040 -0.001841  0.006298 -0.025374  0.150416 -0.026127\n",
      "1   0.035045 -0.004188  0.007030  0.089009  0.173781 -0.125078\n",
      "2  -0.105220 -0.011863  0.000660  0.007329  0.278349  0.001860\n",
      "3  -0.009950 -0.111413  0.003794  0.016005  0.040921 -0.028623\n",
      "4   0.045969  0.005730 -0.081212 -0.021229  0.106664  0.146152\n",
      "5  -0.191290 -0.007822 -0.001293 -0.000661  0.085691  0.029356\n",
      "6   0.031039  0.017537 -0.088199  0.023213  0.006116  0.169801\n",
      "7  -0.080908  0.029750 -0.078277  0.017354  0.000904 -0.109658\n",
      "8   0.004788  0.015272  0.003337  0.036492 -0.010649  0.078096\n",
      "9   0.054778  0.067324 -0.131851 -0.004394 -0.003575 -0.046052\n",
      "10  0.055949 -0.070308 -0.036557 -0.026545 -0.043551 -0.092511\n",
      "11 -0.092488 -0.062544  0.013799 -0.036672  0.043587 -0.142087\n",
      "12 -0.230415 -0.006029  0.039471  0.002229 -0.015309  0.124376\n",
      "13 -0.060432 -0.023558  0.060695 -0.012156  0.000138 -0.067454\n",
      "14 -0.077862 -0.020735  0.000987 -0.004296  0.000498 -0.202422\n",
      "15  0.102989 -0.057218  0.022474  0.000150 -0.017891 -0.487323\n",
      "16 -0.051421 -0.093514  0.012283  0.016626 -0.032512 -0.127908\n",
      "17 -0.161340 -0.230047  0.054911  0.015826 -0.045840 -0.230695\n",
      "18 -0.121099 -0.172188  0.008176 -0.006166 -0.088738  0.198386\n",
      "19  0.171942 -0.074761  0.009511 -0.033838 -0.072991  0.149972\n",
      "      ch7imp    ch8imp    ch9imp   ch10imp   ch12imp   ch13imp\n",
      "0   0.016040 -0.001841  0.006298 -0.025374  0.150416 -0.026127\n",
      "1   0.035045 -0.004188  0.007030  0.089009  0.173781 -0.125078\n",
      "2  -0.105220 -0.011863  0.000660  0.007329  0.278349  0.001860\n",
      "3  -0.009950 -0.111413  0.003794  0.016005  0.040921 -0.028623\n",
      "4   0.045969  0.005730 -0.081212 -0.021229  0.106664  0.146152\n",
      "5  -0.191290 -0.007822 -0.001293 -0.000661  0.085691  0.029356\n",
      "6   0.031039  0.017537 -0.088199  0.023213  0.006116  0.169801\n",
      "7  -0.080908  0.029750 -0.078277  0.017354  0.000904 -0.109658\n",
      "8   0.004788  0.015272  0.003337  0.036492 -0.010649  0.078096\n",
      "9   0.054778  0.067324 -0.131851 -0.004394 -0.003575 -0.046052\n",
      "10  0.055949 -0.070308 -0.036557 -0.026545 -0.043551 -0.092511\n",
      "11 -0.092488 -0.062544  0.013799 -0.036672  0.043587 -0.142087\n",
      "12 -0.230415 -0.006029  0.039471  0.002229 -0.015309  0.124376\n",
      "13 -0.060432 -0.023558  0.060695 -0.012156  0.000138 -0.067454\n",
      "14 -0.077862 -0.020735  0.000987 -0.004296  0.000498 -0.202422\n",
      "15  0.102989 -0.057218  0.022474  0.000150 -0.017891 -0.487323\n",
      "16 -0.051421 -0.093514  0.012283  0.016626 -0.032512 -0.127908\n",
      "17 -0.161340 -0.230047  0.054911  0.015826 -0.045840 -0.230695\n",
      "18 -0.121099 -0.172188  0.008176 -0.006166 -0.088738  0.198386\n",
      "19  0.171942 -0.074761  0.009511 -0.033838 -0.072991  0.149972\n",
      " =====================================================================================================\n",
      "Treino e Teste prontos\n",
      " =====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "read = True\n",
    "\n",
    "if read:        \n",
    "    gdf_list = []\n",
    "    print(\"\")\n",
    "    print(\"Aguarde, o tempo total estimado para a leitura dos arquivos é de \"+\n",
    "          str(int((float(len(paths))*20 )/60))+\" minutos e \"+\n",
    "          str(int((float(len(paths))*20 )%60))+\" segundos.\")\n",
    "    print(\"\")\n",
    "    for path, pathc in zip(paths,pathsc):\n",
    "        print(\"Reading \"+path)\n",
    "        gdf = gd.read_diag(path,pathc)\n",
    "        gdf_list.append(gdf)\n",
    "\n",
    "    print(gdf_list)\n",
    "    \n",
    "    separator = \" =====================================================================================================\"\n",
    "\n",
    "    ch1omf = {'omf': []}\n",
    "    ch2omf = {'omf': []}\n",
    "    ch3omf = {'omf': []}\n",
    "    ch4omf = {'omf': []}\n",
    "    ch5omf = {'omf': []}\n",
    "    ch6omf = {'omf': []}\n",
    "    ch7omf = {'omf': []}\n",
    "    ch8omf = {'omf': []}\n",
    "    ch9omf = {'omf': []}\n",
    "    ch10omf = {'omf': []}\n",
    "    ch11omf = {'omf': []}\n",
    "    ch12omf = {'omf': []}\n",
    "    ch13omf = {'omf': []}\n",
    "    ch14omf = {'omf': []}\n",
    "    ch15omf = {'omf': []}\n",
    "    \n",
    "    ch1 = {'omf': [], 'oma': [], 'obs': [], 'dfs': []}\n",
    "    ch2 = {'omf': [], 'oma': [], 'obs': [], 'dfs': []}\n",
    "    ch3 = {'omf': [], 'oma': [], 'obs': [], 'dfs': []}\n",
    "    ch4 = {'omf': [], 'oma': [], 'obs': [], 'dfs': []}\n",
    "    ch5 = {'omf': [], 'oma': [], 'obs': [], 'dfs': []}\n",
    "    ch6 = {'omf': [], 'oma': [], 'obs': [], 'dfs': []}\n",
    "    ch7 = {'omf': [], 'oma': [], 'obs': [], 'dfs': []}\n",
    "    ch8 = {'omf': [], 'oma': [], 'obs': [], 'dfs': []}\n",
    "    ch9 = {'omf': [], 'oma': [], 'obs': [], 'dfs': []}\n",
    "    ch10 = {'omf': [], 'oma': [], 'obs': [], 'dfs': []}\n",
    "    ch11 = {'omf': [], 'oma': [], 'obs': [], 'dfs': []}\n",
    "    ch12 = {'omf': [], 'oma': [], 'obs': [], 'dfs': []}\n",
    "    ch13 = {'omf': [], 'oma': [], 'obs': [], 'dfs': []}\n",
    "    ch14 = {'omf': [], 'oma': [], 'obs': [], 'dfs': []}\n",
    "    ch15 = {'omf': [], 'oma': [], 'obs': [], 'dfs': []}\n",
    "    \n",
    "    ch1imp = {'imp': []}\n",
    "    ch2imp = {'imp': []}\n",
    "    ch3imp = {'imp': []}\n",
    "    ch4imp = {'imp': []}\n",
    "    ch5imp = {'imp': []}\n",
    "    ch6imp = {'imp': []}\n",
    "    ch7imp = {'imp': []}\n",
    "    ch8imp = {'imp': []}\n",
    "    ch9imp = {'imp': []}\n",
    "    ch10imp = {'imp': []}\n",
    "    ch11imp = {'imp': []}\n",
    "    ch12imp = {'imp': []}\n",
    "    ch13imp = {'imp': []}\n",
    "    ch14imp = {'imp': []}\n",
    "    ch15imp = {'imp': []}\n",
    "    \n",
    "    for objeto in gdf_list:\n",
    "        \n",
    "        print(separator)\n",
    "        print(\"Separando dados do arquivo\" + str(objeto._diagFile))\n",
    "        print(separator)\n",
    "        \n",
    "        dados_dict = {\n",
    "                'lat': objeto.obsInfo[varName].loc[varType].lat,\n",
    "                'lon': objeto.obsInfo[varName].loc[varType].lon,\n",
    "                'elev': objeto.obsInfo[varName].loc[varType].elev,\n",
    "                'nchan': objeto.obsInfo[varName].loc[varType].nchan,\n",
    "                'time': objeto.obsInfo[varName].loc[varType].time,\n",
    "                'iuse': objeto.obsInfo[varName].loc[varType].iuse,\n",
    "                'idqc': objeto.obsInfo[varName].loc[varType].idqc,\n",
    "                'inverr': objeto.obsInfo[varName].loc[varType].inverr,\n",
    "                'oer': objeto.obsInfo[varName].loc[varType].oer,\n",
    "                'obs': objeto.obsInfo[varName].loc[varType].obs,\n",
    "                'omf': objeto.obsInfo[varName].loc[varType].omf,\n",
    "                'omf_nobc': objeto.obsInfo[varName].loc[varType].omf_nobc,\n",
    "                'emiss': objeto.obsInfo[varName].loc[varType].emiss,\n",
    "                'oma': objeto.obsInfo[varName].loc[varType].oma,\n",
    "                'oma_nobc': objeto.obsInfo[varName].loc[varType].oma_nobc,\n",
    "                'imp': objeto.obsInfo[varName].loc[varType].imp,\n",
    "                'dfs': objeto.obsInfo[varName].loc[varType].dfs\n",
    "                }\n",
    "        \n",
    "        i=0\n",
    "        #for i in range(int(float(len(objeto.obsInfo[varName].loc[varType].iuse))/1000)):\n",
    "        while i < 151:\n",
    "        \n",
    "            if objeto.obsInfo[varName].loc[varType].iuse[i] == 1:\n",
    "                \n",
    "                if objeto.obsInfo[varName].loc[varType].nchan[i] == 1:\n",
    "                    ch1omf['omf'].append(objeto.obsInfo[varName].loc[varType].omf[i])\n",
    "                    ch1['oma'].append(objeto.obsInfo[varName].loc[varType].oma[i])\n",
    "                    ch1['obs'].append(objeto.obsInfo[varName].loc[varType].obs[i])\n",
    "                    ch1['dfs'].append(objeto.obsInfo[varName].loc[varType].dfs[i])\n",
    "                    ch1imp['imp'].append(objeto.obsInfo[varName].loc[varType].imp[i])\n",
    "                if objeto.obsInfo[varName].loc[varType].nchan[i] == 2:\n",
    "                    ch2omf['omf'].append(objeto.obsInfo[varName].loc[varType].omf[i])\n",
    "                    ch2['oma'].append(objeto.obsInfo[varName].loc[varType].oma[i])\n",
    "                    ch2['obs'].append(objeto.obsInfo[varName].loc[varType].obs[i])\n",
    "                    ch2['dfs'].append(objeto.obsInfo[varName].loc[varType].dfs[i])\n",
    "                    ch2imp['imp'].append(objeto.obsInfo[varName].loc[varType].imp[i])\n",
    "                if objeto.obsInfo[varName].loc[varType].nchan[i] == 3:\n",
    "                    ch3omf['omf'].append(objeto.obsInfo[varName].loc[varType].omf[i])\n",
    "                    ch3['oma'].append(objeto.obsInfo[varName].loc[varType].oma[i])\n",
    "                    ch3['obs'].append(objeto.obsInfo[varName].loc[varType].obs[i])\n",
    "                    ch3['dfs'].append(objeto.obsInfo[varName].loc[varType].dfs[i])\n",
    "                    ch3imp['imp'].append(objeto.obsInfo[varName].loc[varType].imp[i])\n",
    "                if objeto.obsInfo[varName].loc[varType].nchan[i] == 4:\n",
    "                    ch4omf['omf'].append(objeto.obsInfo[varName].loc[varType].omf[i])\n",
    "                    ch4['oma'].append(objeto.obsInfo[varName].loc[varType].oma[i])\n",
    "                    ch4['obs'].append(objeto.obsInfo[varName].loc[varType].obs[i])\n",
    "                    ch4['dfs'].append(objeto.obsInfo[varName].loc[varType].dfs[i])\n",
    "                    ch4imp['imp'].append(objeto.obsInfo[varName].loc[varType].imp[i])\n",
    "                if objeto.obsInfo[varName].loc[varType].nchan[i] == 5:\n",
    "                    ch5omf['omf'].append(objeto.obsInfo[varName].loc[varType].omf[i])\n",
    "                    ch5['oma'].append(objeto.obsInfo[varName].loc[varType].oma[i])\n",
    "                    ch5['obs'].append(objeto.obsInfo[varName].loc[varType].obs[i])\n",
    "                    ch5['dfs'].append(objeto.obsInfo[varName].loc[varType].dfs[i])\n",
    "                    ch5imp['imp'].append(objeto.obsInfo[varName].loc[varType].imp[i])\n",
    "                if objeto.obsInfo[varName].loc[varType].nchan[i] == 6:\n",
    "                    ch6omf['omf'].append(objeto.obsInfo[varName].loc[varType].omf[i])\n",
    "                    ch6['oma'].append(objeto.obsInfo[varName].loc[varType].oma[i])\n",
    "                    ch6['obs'].append(objeto.obsInfo[varName].loc[varType].obs[i])\n",
    "                    ch6['dfs'].append(objeto.obsInfo[varName].loc[varType].dfs[i])\n",
    "                    ch6imp['imp'].append(objeto.obsInfo[varName].loc[varType].imp[i])\n",
    "                if objeto.obsInfo[varName].loc[varType].nchan[i] == 7:\n",
    "                    ch7omf['omf'].append(objeto.obsInfo[varName].loc[varType].omf[i])\n",
    "                    ch7['oma'].append(objeto.obsInfo[varName].loc[varType].oma[i])\n",
    "                    ch7['obs'].append(objeto.obsInfo[varName].loc[varType].obs[i])\n",
    "                    ch7['dfs'].append(objeto.obsInfo[varName].loc[varType].dfs[i])\n",
    "                    ch7imp['imp'].append(objeto.obsInfo[varName].loc[varType].imp[i])\n",
    "                if objeto.obsInfo[varName].loc[varType].nchan[i] == 8:\n",
    "                    ch8omf['omf'].append(objeto.obsInfo[varName].loc[varType].omf[i])\n",
    "                    ch8['oma'].append(objeto.obsInfo[varName].loc[varType].oma[i])\n",
    "                    ch8['obs'].append(objeto.obsInfo[varName].loc[varType].obs[i])\n",
    "                    ch8['dfs'].append(objeto.obsInfo[varName].loc[varType].dfs[i])\n",
    "                    ch8imp['imp'].append(objeto.obsInfo[varName].loc[varType].imp[i])\n",
    "                if objeto.obsInfo[varName].loc[varType].nchan[i] == 9:\n",
    "                    ch9omf['omf'].append(objeto.obsInfo[varName].loc[varType].omf[i])\n",
    "                    ch9['oma'].append(objeto.obsInfo[varName].loc[varType].oma[i])\n",
    "                    ch9['obs'].append(objeto.obsInfo[varName].loc[varType].obs[i])\n",
    "                    ch9['dfs'].append(objeto.obsInfo[varName].loc[varType].dfs[i])\n",
    "                    ch9imp['imp'].append(objeto.obsInfo[varName].loc[varType].imp[i])\n",
    "                if objeto.obsInfo[varName].loc[varType].nchan[i] == 10:\n",
    "                    ch10omf['omf'].append(objeto.obsInfo[varName].loc[varType].omf[i])\n",
    "                    ch10['oma'].append(objeto.obsInfo[varName].loc[varType].oma[i])\n",
    "                    ch10['obs'].append(objeto.obsInfo[varName].loc[varType].obs[i])\n",
    "                    ch10['dfs'].append(objeto.obsInfo[varName].loc[varType].dfs[i])\n",
    "                    ch10imp['imp'].append(objeto.obsInfo[varName].loc[varType].imp[i])\n",
    "                if objeto.obsInfo[varName].loc[varType].nchan[i] == 11:\n",
    "                    ch11omf['omf'].append(objeto.obsInfo[varName].loc[varType].omf[i])\n",
    "                    ch11['oma'].append(objeto.obsInfo[varName].loc[varType].oma[i])\n",
    "                    ch11['obs'].append(objeto.obsInfo[varName].loc[varType].obs[i])\n",
    "                    ch11['dfs'].append(objeto.obsInfo[varName].loc[varType].dfs[i])\n",
    "                    ch11imp['imp'].append(objeto.obsInfo[varName].loc[varType].imp[i])\n",
    "                if objeto.obsInfo[varName].loc[varType].nchan[i] == 12:\n",
    "                    ch12omf['omf'].append(objeto.obsInfo[varName].loc[varType].omf[i])\n",
    "                    ch12['oma'].append(objeto.obsInfo[varName].loc[varType].oma[i])\n",
    "                    ch12['obs'].append(objeto.obsInfo[varName].loc[varType].obs[i])\n",
    "                    ch12['dfs'].append(objeto.obsInfo[varName].loc[varType].dfs[i])\n",
    "                    ch12imp['imp'].append(objeto.obsInfo[varName].loc[varType].imp[i])\n",
    "                if objeto.obsInfo[varName].loc[varType].nchan[i] == 13:\n",
    "                    ch13omf['omf'].append(objeto.obsInfo[varName].loc[varType].omf[i])\n",
    "                    ch13['oma'].append(objeto.obsInfo[varName].loc[varType].oma[i])\n",
    "                    ch13['obs'].append(objeto.obsInfo[varName].loc[varType].obs[i])\n",
    "                    ch13['dfs'].append(objeto.obsInfo[varName].loc[varType].dfs[i])\n",
    "                    ch13imp['imp'].append(objeto.obsInfo[varName].loc[varType].imp[i])\n",
    "                if objeto.obsInfo[varName].loc[varType].nchan[i] == 14:\n",
    "                    ch14omf['omf'].append(objeto.obsInfo[varName].loc[varType].omf[i])\n",
    "                    ch14['oma'].append(objeto.obsInfo[varName].loc[varType].oma[i])\n",
    "                    ch14['obs'].append(objeto.obsInfo[varName].loc[varType].obs[i])\n",
    "                    ch14['dfs'].append(objeto.obsInfo[varName].loc[varType].dfs[i])\n",
    "                    ch14imp['imp'].append(objeto.obsInfo[varName].loc[varType].imp[i])\n",
    "                if objeto.obsInfo[varName].loc[varType].nchan[i] == 15:\n",
    "                    ch15omf['omf'].append(objeto.obsInfo[varName].loc[varType].omf[i])\n",
    "                    ch15['oma'].append(objeto.obsInfo[varName].loc[varType].oma[i])\n",
    "                    ch15['obs'].append(objeto.obsInfo[varName].loc[varType].obs[i])\n",
    "                    ch15['dfs'].append(objeto.obsInfo[varName].loc[varType].dfs[i])\n",
    "                    ch15imp['imp'].append(objeto.obsInfo[varName].loc[varType].imp[i])    \n",
    "                    \n",
    "            i+=1\n",
    "        \"\"\"print(\"Canal 1: \" + str(ch1))\n",
    "        print(separator)\n",
    "        print(\"Canal 2: \" + str(ch2))\n",
    "        print(separator)\n",
    "        print(\"Canal 3: \" + str(ch3))\n",
    "        print(separator)\n",
    "        print(\"Canal 4: \" + str(ch4))\n",
    "        print(separator)\n",
    "        print(\"Canal 5: \" + str(ch5))\n",
    "        print(separator)\n",
    "        print(\"Canal 6: \" + str(ch6))\n",
    "        print(separator)\n",
    "        print(\"Canal 7: \" + str(ch7))\n",
    "        print(separator)\n",
    "        print(\"Canal 8: \" + str(ch8))\n",
    "        print(separator)\n",
    "        print(\"Canal 9: \" + str(ch9))\n",
    "        print(separator)\n",
    "        print(\"Canal 10: \" + str(ch10))\n",
    "        print(separator)\n",
    "        print(\"Canal 11: \" + str(ch11))\n",
    "        print(separator)\n",
    "        print(\"Canal 12: \" + str(ch12))\n",
    "        print(separator)\n",
    "        print(\"Canal 13: \" + str(ch13))\n",
    "        print(separator)\n",
    "        print(\"Canal 14: \" + str(ch14))\n",
    "        print(separator)\n",
    "        print(\"Canal 15: \" + str(ch15))\"\"\"\n",
    "            \n",
    "        \n",
    "    print(\"Done!\")\n",
    "    \n",
    "    #//////////\n",
    "    \"\"\"\n",
    "    canais = {'ch1': ch1, 'ch2': ch2, 'ch3': ch3, 'ch4': ch4, 'ch5': ch5,\n",
    "              'ch6': ch6, 'ch7': ch7, 'ch8': ch8, 'ch9': ch9, 'ch10': ch10,\n",
    "              'ch11': ch11, 'ch12': ch12, 'ch13': ch13, 'ch14': ch14, 'ch15': ch15}\n",
    "    \n",
    "    canais_df = pd.DataFrame(canais)\n",
    "    \n",
    "    canaisimp = {'ch1imp': ch1imp['imp'], 'ch2imp': ch2imp['imp'], 'ch3imp': ch3imp['imp'], \n",
    "                 'ch4imp': ch4imp['imp'], 'ch5imp': ch5imp['imp'], 'ch6imp': ch6imp['imp'],  \n",
    "                 'ch7imp': ch7imp['imp'], 'ch8imp': ch8imp['imp'], 'ch9imp': ch9imp['imp'], \n",
    "                 'ch10imp': ch10imp['imp'], 'ch11imp': ch11imp['imp'], 'ch12imp': ch12imp['imp'], \n",
    "                 'ch13imp': ch13imp['imp'], 'ch14imp': ch14imp['imp'], 'ch15imp': ch15imp['imp']}\n",
    "    print(canaisimp)\n",
    "    \n",
    "    canaisimp_df = pd.DataFrame(canaisimp)\n",
    "    \n",
    "    canaisomf = {'ch1omf': ch1omf['omf'], 'ch2omf': ch2omf['omf'], 'ch3omf': ch3omf['omf'], \n",
    "                 'ch4omf': ch4omf['omf'], 'ch5omf': ch5omf['omf'], 'ch6omf': ch6omf['omf'], \n",
    "                 'ch7omf': ch7omf['omf'], 'ch8omf': ch8omf['omf'], 'ch9omf': ch9omf['omf'], \n",
    "                 'ch10omf': ch10omf['omf'], 'ch11omf': ch11omf['omf'], 'ch12omf': ch12omf['omf'], \n",
    "                 'ch13omf': ch13omf['omf'], 'ch14omf': ch14omf['omf'], 'ch15omf': ch15omf['omf']}\n",
    "    \n",
    "    canaisomf_df = pd.DataFrame(canaisomf)\n",
    "    \"\"\"\n",
    "    #//////////  \n",
    "    \n",
    "    canais = {'ch7': ch7imp['imp'], 'ch8': ch8imp['imp'], 'ch9': ch9imp['imp'], \n",
    "              'ch10': ch10imp['imp'],\n",
    "              'ch12': ch12imp['imp'], 'ch13': ch13imp['imp']}\n",
    "    \n",
    "    canais_df = pd.DataFrame(canais)\n",
    "    \n",
    "    canaisimp = {'ch7imp': ch7imp['imp'], 'ch8imp': ch8imp['imp'], 'ch9imp': ch9imp['imp'], \n",
    "                 'ch10imp': ch10imp['imp'], 'ch12imp': ch12imp['imp'], \n",
    "                 'ch13imp': ch13imp['imp']}\n",
    "    print(canaisimp)\n",
    "    \n",
    "    canaisimp_df = pd.DataFrame(canaisimp)\n",
    "    \n",
    "    canaisomf = {'ch7omf': ch7omf['omf'], 'ch8omf': ch8omf['omf'], 'ch9omf': ch9omf['omf'], \n",
    "                 'ch10omf': ch10omf['omf'], 'ch12omf': ch12omf['omf'], \n",
    "                 'ch13omf': ch13omf['omf']}\n",
    "    \n",
    "    canaisomf_df = pd.DataFrame(canaisomf)\n",
    "    \n",
    "    #/////////    \n",
    "\n",
    "    #canais_df.dropna(inplace=True)\n",
    "    #canaisimp_df = canaisimp_df.loc[canais_df.index]\n",
    "    \n",
    "    print(canais_df)\n",
    "    print(canaisimp_df)\n",
    "    \n",
    "    print(separator)\n",
    "\n",
    "    # Dividir os dados em conjuntos de treinamento e teste\n",
    "    X_train, X_test, y_train, y_test = train_test_split(canais_df, canaisimp_df, \n",
    "                                                        test_size=0.2, random_state=42)\n",
    "    print('Treino e Teste prontos')\n",
    "    print(separator)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88debeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e30bd27d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo treinado!\n",
      " =====================================================================================================\n",
      "Mean Absolute Error: 0.07607030692056772\n",
      "Mean Squared Error: 0.01320244252598208\n",
      "Root Mean Squared Error: 0.11490188216901445\n",
      "R^2 Score: -0.4086748181291681\n",
      "Melhores canais: ['ch13', 'ch7', 'ch12', 'ch8', 'ch9', 'ch10']\n"
     ]
    }
   ],
   "source": [
    "# Treinar o modelo\n",
    "modelo = RandomForestRegressor()\n",
    "modelo.fit(X_train, y_train)\n",
    "print('Modelo treinado!')\n",
    "print(separator)\n",
    "      \n",
    "# Fazer previsões\n",
    "y_pred = modelo.predict(X_test)\n",
    "\n",
    "# Avaliar o modelo\n",
    "#atriz_confusao = confusion_matrix(y_test, y_pred)\n",
    "#elatorio_classificacao = classification_report(y_test, y_pred)\n",
    "\n",
    "#rint(\"Matriz de Confusão:\")\n",
    "#rint(matriz_confusao)\n",
    "\n",
    "# Calcular métricas de avaliação\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "print(\"R^2 Score:\", r2)\n",
    "\n",
    "#print(\"\\nRelatório de Classificação:\")\n",
    "#print(relatorio_classificacao)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Obter a importância das características\n",
    "importancias = modelo.feature_importances_\n",
    "\n",
    "# Mapear o índice das características para os nomes dos canais\n",
    "importancias_por_canal = dict(zip(range(len(X_train.columns)), X_train.columns))\n",
    "\n",
    "# Classificar os canais por importância\n",
    "canais_ordenados = sorted(importancias_por_canal.items(), key=lambda x: importancias[x[0]], reverse=True)\n",
    "\n",
    "# Selecionar os top n canais com maior importância\n",
    "n = 6  # Número de canais que você deseja selecionar\n",
    "melhores_canais = [canal[1] for canal in canais_ordenados[:n]]\n",
    "\n",
    "print(\"Melhores canais:\", melhores_canais)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dabd9ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " =====================================================================================================\n",
      "Separando dados do arquivo/home/patrick/readDiag/data/2020010106/diag_amsua_n15_01.2020010106\n",
      " =====================================================================================================\n",
      " =====================================================================================================\n",
      "Separando dados do arquivo/home/patrick/readDiag/data/2020010100/diag_amsua_n15_01.2020010100\n",
      " =====================================================================================================\n",
      "        nchan       omf       oma         obs       dfs\n",
      "points                                                 \n",
      "5         6.0  0.048127 -0.016341  232.509995 -0.011228\n",
      "6         7.0 -0.028671 -0.121734  231.570007  0.010665\n",
      "7         8.0  0.143730  0.036381  232.029999 -0.056107\n",
      "8         9.0  0.117148  0.035975  234.600006 -0.027968\n",
      "9        10.0  0.132834  0.083827  237.660004 -0.016275\n",
      "...       ...       ...       ...         ...       ...\n",
      "97387     8.0 -0.396553 -0.210560  201.979996 -0.268203\n",
      "97388     9.0 -0.313287  0.040100  194.210007 -0.325621\n",
      "97389    10.0 -0.418757  0.038911  190.679993 -0.479124\n",
      "97391    12.0 -0.166651  0.321933  199.119995 -0.081405\n",
      "97392    13.0 -0.561209  0.328142  217.080002 -0.332377\n",
      "\n",
      "[44617 rows x 5 columns]\n",
      "             imp\n",
      "points          \n",
      "5      -0.007416\n",
      "6       0.055949\n",
      "7      -0.070308\n",
      "8      -0.036557\n",
      "9      -0.026545\n",
      "...          ...\n",
      "97387  -0.410613\n",
      "97388  -0.283942\n",
      "97389  -0.434604\n",
      "97391   0.075852\n",
      "97392  -0.138034\n",
      "\n",
      "[44617 rows x 1 columns]\n",
      " =====================================================================================================\n",
      "Treino e Teste prontos\n",
      " =====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/patrick/anaconda3/envs/readDiag/lib/python3.7/site-packages/ipykernel_launcher.py:74: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo treinado!\n",
      " =====================================================================================================\n",
      "Mean Absolute Error: 0.009045949836182943\n",
      "Mean Squared Error: 0.0032608711980366177\n",
      "Root Mean Squared Error: 0.05710403836889837\n",
      "R^2 Score: 0.9843445121839279\n",
      "Melhores canais: ['dfs', 'oma', 'omf']\n"
     ]
    }
   ],
   "source": [
    "X = {'nchan': [], 'omf': [], 'oma': [], 'obs': [], 'dfs': []}\n",
    "y = {'imp': []}\n",
    "for arquivo in gdf_list:\n",
    "        \n",
    "        print(separator)\n",
    "        print(\"Separando dados do arquivo\" + str(objeto._diagFile))\n",
    "        print(separator)\n",
    "        \n",
    "        objeto = arquivo\n",
    "        \n",
    "        nome_arquivo_csv = str(objeto._diagFile)\n",
    "        \n",
    "        dados_dict = {\n",
    "                'lat': objeto.obsInfo[varName].loc[varType].lat,\n",
    "                'lon': objeto.obsInfo[varName].loc[varType].lon,\n",
    "                'elev': objeto.obsInfo[varName].loc[varType].elev,\n",
    "                'nchan': objeto.obsInfo[varName].loc[varType].nchan,\n",
    "                'time': objeto.obsInfo[varName].loc[varType].time,\n",
    "                'iuse': objeto.obsInfo[varName].loc[varType].iuse,\n",
    "                'idqc': objeto.obsInfo[varName].loc[varType].idqc,\n",
    "                'inverr': objeto.obsInfo[varName].loc[varType].inverr,\n",
    "                'oer': objeto.obsInfo[varName].loc[varType].oer,\n",
    "                'obs': objeto.obsInfo[varName].loc[varType].obs,\n",
    "                'omf': objeto.obsInfo[varName].loc[varType].omf,\n",
    "                'omf_nobc': objeto.obsInfo[varName].loc[varType].omf_nobc,\n",
    "                'emiss': objeto.obsInfo[varName].loc[varType].emiss,\n",
    "                'oma': objeto.obsInfo[varName].loc[varType].oma,\n",
    "                'oma_nobc': objeto.obsInfo[varName].loc[varType].oma_nobc,\n",
    "                'imp': objeto.obsInfo[varName].loc[varType].imp,\n",
    "                'dfs': objeto.obsInfo[varName].loc[varType].dfs\n",
    "                }\n",
    "        \n",
    "        # Adicionar dados de entrada (X)\n",
    "        #X.append(objeto.obsInfo[varName].loc[varType].nchan + objeto.obsInfo[varName].loc[varType].oma + \n",
    "        #                    objeto.obsInfo[varName].loc[varType].omf + objeto.obsInfo[varName].loc[varType].dfs\n",
    "        #                   )\n",
    "        X['nchan'] = objeto.obsInfo[varName].loc[varType].nchan\n",
    "        X['omf'] = objeto.obsInfo[varName].loc[varType].omf\n",
    "        X['oma'] = objeto.obsInfo[varName].loc[varType].oma\n",
    "        X['obs'] = objeto.obsInfo[varName].loc[varType].obs\n",
    "        X['dfs'] = objeto.obsInfo[varName].loc[varType].dfs\n",
    "        # Adicionar dados de saída (y)\n",
    "        #y.append(objeto.obsInfo[varName].loc[varType].imp)\n",
    "        y['imp'] = objeto.obsInfo[varName].loc[varType].imp\n",
    "        \n",
    "# Criar DataFrames finais\n",
    "#y['imp'] = y['imp'].dropna()\n",
    "\n",
    "X_df = pd.DataFrame(X)\n",
    "y_df = pd.DataFrame(y)\n",
    "\n",
    "X_df.dropna(inplace=True)\n",
    "y_df = y_df.loc[X_df.index]\n",
    "\n",
    "print(X_df)\n",
    "print(y_df)\n",
    "\n",
    "#print(X)\n",
    "#print(y)\n",
    "#print(X.head())\n",
    "#print(y.head())\n",
    "\n",
    "#print(X.shape, y.shape)\n",
    "\n",
    "print(separator)\n",
    "\n",
    "# Dividir os dados em conjuntos de treinamento e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_df, y_df, test_size=0.2, random_state=42)\n",
    "print('Treino e Teste prontos')\n",
    "print(separator)\n",
    "\n",
    "# Treinar o modelo\n",
    "modelo = RandomForestRegressor()\n",
    "modelo.fit(X_train, y_train)\n",
    "print('Modelo treinado!')\n",
    "print(separator)\n",
    "      \n",
    "# Fazer previsões\n",
    "y_pred = modelo.predict(X_test)\n",
    "\n",
    "# Avaliar o modelo\n",
    "#atriz_confusao = confusion_matrix(y_test, y_pred)\n",
    "#elatorio_classificacao = classification_report(y_test, y_pred)\n",
    "\n",
    "#rint(\"Matriz de Confusão:\")\n",
    "#rint(matriz_confusao)\n",
    "\n",
    "# Calcular métricas de avaliação\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "print(\"R^2 Score:\", r2)\n",
    "\n",
    "#print(\"\\nRelatório de Classificação:\")\n",
    "#print(relatorio_classificacao)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Obter a importância das características\n",
    "importancias = modelo.feature_importances_\n",
    "\n",
    "# Mapear o índice das características para os nomes dos canais\n",
    "importancias_por_canal = dict(zip(range(len(X_train.columns)), X_train.columns))\n",
    "\n",
    "# Classificar os canais por importância\n",
    "canais_ordenados = sorted(importancias_por_canal.items(), key=lambda x: importancias[x[0]], reverse=True)\n",
    "\n",
    "# Selecionar os top n canais com maior importância\n",
    "n = 3  # Número de canais que você deseja selecionar\n",
    "melhores_canais = [canal[1] for canal in canais_ordenados[:n]]\n",
    "\n",
    "print(\"Melhores canais:\", melhores_canais)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded2ee17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
