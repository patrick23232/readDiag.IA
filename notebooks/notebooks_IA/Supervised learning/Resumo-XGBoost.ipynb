{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcd498e0-9049-4c7a-b5bb-04a59a016665",
   "metadata": {},
   "source": [
    "<img src=\"logoINPE.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f65c2c3-9171-42fd-a166-d301d8f77d1e",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21741bf7-6116-449c-8231-6e2e8a5acaea",
   "metadata": {},
   "source": [
    "**Definição:**\n",
    "XGBoost (Extreme Gradient Boosting) é uma biblioteca otimizada de aprendizado de máquina que implementa o algoritmo de boosting de gradiente. Criada por Tianqi Chen, ela é conhecida por sua eficiência, desempenho e capacidade de ajuste. XGBoost tem sido amplamente adotada em competições de ciência de dados e aplicações no mundo real devido ao seu poder de previsão e flexibilidade.\n",
    "\n",
    "### Estrutura e Funcionamento\n",
    "\n",
    "**Boosting de Gradiente:**\n",
    "Boosting de gradiente é uma técnica de ensemble que combina a previsão de vários modelos fracos (geralmente árvores de decisão) para formar um modelo forte. O processo é iterativo e cada novo modelo é treinado para corrigir os erros dos modelos anteriores.\n",
    "\n",
    "**Componentes Principais do XGBoost:**\n",
    "\n",
    "1. **Árvores de Decisão:**\n",
    "   - Modelos base utilizados no XGBoost. Elas são construídas sequencialmente, com cada árvore tentando corrigir os erros da anterior.\n",
    "\n",
    "2. **Função de Objetivo:**\n",
    "   - Combina a função de perda (que mede o quão bem o modelo está se saindo) e o termo de regularização (que penaliza a complexidade do modelo para evitar o sobreajuste). A função de objetivo é minimizada durante o treinamento.\n",
    "\n",
    "**Processo de Treinamento:**\n",
    "1. **Inicialização:**\n",
    "   - Começa com uma previsão inicial, geralmente a média dos valores alvo no caso de regressão.\n",
    "\n",
    "2. **Adição de Árvores:**\n",
    "   - Em cada iteração, uma nova árvore é adicionada para corrigir os erros das previsões anteriores.\n",
    "   - A função de perda é derivada para encontrar a direção e magnitude dos ajustes necessários.\n",
    "   - O algoritmo ajusta os pesos dos exemplos de treinamento para focar nos exemplos mais difíceis.\n",
    "\n",
    "3. **Regularização:**\n",
    "   - Regularização é aplicada para evitar o sobreajuste, penalizando árvores muito complexas.\n",
    "\n",
    "4. **Parada:**\n",
    "   - O treinamento continua até que um número predefinido de árvores tenha sido adicionada ou uma condição de parada precoce (early stopping) seja satisfeita.\n",
    "\n",
    "### Vantagens do XGBoost\n",
    "\n",
    "1. **Eficiência e Velocidade:**\n",
    "   - Implementação otimizada para velocidade e uso eficiente de recursos computacionais.\n",
    "   - Suporte a paralelismo e execução distribuída.\n",
    "\n",
    "2. **Precisão:**\n",
    "   - Alto desempenho preditivo, frequentemente superando outros algoritmos de aprendizado de máquina em competições e benchmarks.\n",
    "\n",
    "3. **Flexibilidade:**\n",
    "   - Suporta várias funções de perda personalizáveis, o que o torna aplicável a uma ampla gama de problemas (regressão, classificação, ranking, etc.).\n",
    "\n",
    "4. **Regularização:**\n",
    "   - Técnicas avançadas de regularização para evitar sobreajuste, como L1 e L2.\n",
    "\n",
    "5. **Suporte a Dados Faltantes:**\n",
    "   - Capacidade de lidar com valores faltantes automaticamente durante o treinamento.\n",
    "\n",
    "6. **Importância das Características:**\n",
    "   - Fornece medidas de importância das características, ajudando na interpretabilidade do modelo.\n",
    "\n",
    "### Aplicações do XGBoost\n",
    "\n",
    "XGBoost tem sido utilizado em diversas áreas e aplicações, incluindo:\n",
    "- Competição de ciência de dados (como Kaggle).\n",
    "- Finanças (previsão de risco de crédito, detecção de fraude).\n",
    "- Saúde (previsão de doenças, análise de imagens médicas).\n",
    "- Marketing (previsão de churn de clientes, segmentação de mercado).\n",
    "- Manutenção preditiva (previsão de falhas em máquinas e equipamentos).\n",
    "\n",
    "### Conclusão\n",
    "\n",
    "XGBoost é uma poderosa ferramenta de aprendizado de máquina que combina eficiência, precisão e flexibilidade. Sua implementação otimizada e capacidade de ajuste fino de parâmetros o tornam uma escolha popular para cientistas de dados e engenheiros de machine learning. Embora possa exigir uma cuidadosa sintonização de parâmetros para obter os melhores resultados, suas vantagens em termos de desempenho preditivo e capacidade de generalização fazem do XGBoost uma escolha excelente para uma ampla variedade de tarefas de previsão."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f18dcf5-d8ef-4bab-a95e-5a57fc828bd0",
   "metadata": {},
   "source": [
    "# Parâmetros do `XGBoost`:\n",
    "\n",
    "**Parâmetros Gerais:**\n",
    "\n",
    "1. `booster`: Define o tipo de modelo a ser usado. Pode ser \"gbtree\" para árvore de decisão, \"gblinear\" para modelo linear ou \"dart\" para adição de dropout à árvore de decisão.\n",
    "\n",
    "2. `verbosity`: Controla o nível de verbosidade ao imprimir mensagens. Pode ser 0 (silencioso), 1 (informações detalhadas) ou 2 (informações detalhadas e mensagens de depuração).\n",
    "\n",
    "3. `nthread`: Número de threads a serem usados para rodar XGBoost. Se definido como -1, o máximo de threads disponíveis será usado.\n",
    "\n",
    "**Parâmetros de Booster (Árvore):**\n",
    "\n",
    "1. `eta` (ou `learning_rate`): Taxa de aprendizado. Controla a contribuição de cada árvore no modelo.\n",
    "\n",
    "2. `gamma`: Mínima redução da perda necessária para fazer uma nova partição em um nó da árvore.\n",
    "\n",
    "3. `max_depth`: Profundidade máxima de cada árvore. Maior profundidade pode levar a overfitting.\n",
    "\n",
    "4. `min_child_weight`: Soma mínima dos pesos das amostras necessária em um nó para continuar dividindo.\n",
    "\n",
    "5. `max_delta_step`: Limite para a atualização do peso de cada árvore. Pode ajudar a tornar o treinamento mais conservador.\n",
    "\n",
    "6. `subsample`: Fração de observações a serem amostradas aleatoriamente para cada árvore. Reduzir pode evitar overfitting.\n",
    "\n",
    "7. `colsample_bytree`: Fração de features a serem amostradas aleatoriamente para cada árvore.\n",
    "\n",
    "8. `lambda` (ou `reg_lambda`): Parâmetro de regularização L2.\n",
    "\n",
    "9. `alpha` (ou `reg_alpha`): Parâmetro de regularização L1.\n",
    "\n",
    "10. `scale_pos_weight`: Controla o balanceamento de classes para classificação desequilibrada.\n",
    "\n",
    "**Parâmetros de Treinamento:**\n",
    "\n",
    "1. `num_boost_round`: Número de iterações de boosting (número de árvores a serem criadas).\n",
    "\n",
    "2. `early_stopping_rounds`: Se a métrica de avaliação não melhorar por esse número de rodadas, o treinamento será interrompido.\n",
    "\n",
    "3. `eval_metric`: A métrica de avaliação a ser usada.\n",
    "\n",
    "**Parâmetros de Avaliação:**\n",
    "\n",
    "1. `eval_set`: Conjunto de dados de validação a serem usados para avaliação durante o treinamento.\n",
    "\n",
    "2. `verbose_eval`: Controla a verbosidade da saída durante a avaliação.\n",
    "\n",
    "**Parâmetros de Regressão Linear (para `gblinear`):**\n",
    "\n",
    "1. `lambda`: Parâmetro de regularização L2.\n",
    "\n",
    "2. `alpha`: Parâmetro de regularização L1.\n",
    "\n",
    "3. `lambda_bias`: Parâmetro de regularização adicional para o termo de viés.\n",
    "\n",
    "**Parâmetros específicos do Dart:**\n",
    "\n",
    "1. `sample_type`: Tipo de amostra a ser usada. Pode ser \"uniform\" para amostras uniformes ou \"weighted\" para amostras ponderadas.\n",
    "\n",
    "2. `normalize_type`: Tipo de normalização a ser usada. Pode ser \"tree\" para normalização por árvore ou \"forest\" para normalização por floresta.\n",
    "\n",
    "3. `rate_drop`: Fração das árvores a serem eliminadas aleatoriamente a cada iteração.\n",
    "\n",
    "4. `skip_drop`: Probabilidade de que uma árvore seja eliminada aleatoriamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb353ace-6030-48e4-8e3a-c18b00410804",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "readDiag",
   "language": "python",
   "name": "readdiag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
