{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55a28053-8656-4f72-ac2b-a50574293bfc",
   "metadata": {},
   "source": [
    "<img src=\"logoINPE.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f9ce61-614c-4b5b-9306-bb3f9cd93135",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "\n",
    "**Definição:**\n",
    "Random Forest é uma técnica de aprendizado de máquina baseada em ensemble, desenvolvida por Leo Breiman e Adele Cutler. Ela combina múltiplas árvores de decisão para formar um modelo mais robusto e preciso. Cada árvore na floresta é construída a partir de uma amostra aleatória do conjunto de dados e realiza previsões independentes. A previsão final do modelo é determinada pela agregação das previsões individuais das árvores, geralmente por meio de votação (para classificação) ou média (para regressão).\n",
    "\n",
    "### Estrutura e Funcionamento\n",
    "\n",
    "**Árvores de Decisão:**\n",
    "Árvores de decisão são a unidade básica do Random Forest. Elas são modelos não paramétricos que dividem os dados em subconjuntos baseados em características de entrada para fazer previsões.\n",
    "\n",
    "**Componentes Principais do Random Forest:**\n",
    "\n",
    "1. **Bootstrapping:**\n",
    "   - Cada árvore na floresta é treinada em uma amostra aleatória (com reposição) do conjunto de dados original. Este processo é conhecido como bootstrap sampling.\n",
    "\n",
    "2. **Seleção Aleatória de Características:**\n",
    "   - Para cada divisão em uma árvore, um subconjunto aleatório de características é considerado. Isso adiciona uma camada de aleatoriedade e diversidade, ajudando a reduzir a correlação entre as árvores.\n",
    "\n",
    "3. **Crescimento das Árvores:**\n",
    "   - As árvores são crescidas até sua máxima extensão sem poda (pruning), resultando em árvores profundas e altamente variáveis. A falta de poda permite capturar interações complexas nos dados.\n",
    "\n",
    "### Processo de Treinamento:\n",
    "\n",
    "1. **Criação do Conjunto de Treinamento:**\n",
    "   - Para cada árvore, uma amostra bootstrap do conjunto de dados é criada.\n",
    "\n",
    "2. **Construção da Árvore:**\n",
    "   - A árvore é construída a partir da amostra bootstrap, utilizando um subconjunto aleatório de características em cada divisão.\n",
    "\n",
    "3. **Repetição:**\n",
    "   - O processo é repetido para todas as árvores na floresta.\n",
    "\n",
    "4. **Agregação das Previsões:**\n",
    "   - Para classificação, a previsão final é determinada pela votação majoritária das árvores. Para regressão, é calculada a média das previsões das árvores.\n",
    "\n",
    "### Vantagens do Random Forest\n",
    "\n",
    "1. **Redução do Sobreajuste:**\n",
    "   - A aleatoriedade introduzida pelo bootstrap sampling e pela seleção aleatória de características reduz o risco de sobreajuste, que é comum em árvores de decisão individuais.\n",
    "\n",
    "2. **Precisão e Robustez:**\n",
    "   - Combinar múltiplas árvores de decisão melhora a precisão preditiva e a robustez do modelo.\n",
    "\n",
    "3. **Flexibilidade:**\n",
    "   - Aplicável tanto para problemas de classificação quanto de regressão.\n",
    "\n",
    "4. **Importância das Características:**\n",
    "   - Random Forest fornece uma medida de importância das características, que pode ser útil para a interpretação do modelo e a seleção de características.\n",
    "\n",
    "5. **Resistência a Dados Faltantes:**\n",
    "   - O algoritmo pode lidar com dados faltantes de maneira eficaz, estimando valores ausentes com base em critérios de proximidade.\n",
    "\n",
    "6. **Paralelismo:**\n",
    "   - A construção de cada árvore é independente, permitindo a implementação paralela e distribuída.\n",
    "\n",
    "### Desvantagens do Random Forest\n",
    "\n",
    "1. **Complexidade Computacional:**\n",
    "   - Treinar um grande número de árvores pode ser computacionalmente intensivo e exigir muito tempo.\n",
    "\n",
    "2. **Interpretação:**\n",
    "   - Embora forneça medidas de importância das características, a complexidade do modelo torna difícil a interpretação completa das previsões individuais.\n",
    "\n",
    "### Aplicações do Random Forest\n",
    "\n",
    "Random Forest é amplamente utilizado em várias áreas devido à sua flexibilidade e precisão, incluindo:\n",
    "- **Finanças:** Análise de risco de crédito, detecção de fraude.\n",
    "- **Saúde:** Diagnóstico de doenças, análise de dados genômicos.\n",
    "- **Marketing:** Previsão de churn de clientes, segmentação de mercado.\n",
    "- **Ecologia:** Classificação de espécies, previsão de mudanças ambientais.\n",
    "- **Indústria:** Manutenção preditiva, controle de qualidade.\n",
    "\n",
    "### Conclusão\n",
    "\n",
    "Random Forest é uma poderosa técnica de aprendizado de máquina que combina a simplicidade e a interpretabilidade das árvores de decisão com a robustez e a precisão dos métodos de ensemble. Sua capacidade de lidar com dados complexos e fornecer previsões precisas e confiáveis a torna uma escolha popular para uma ampla variedade de aplicações. Com o contínuo avanço das tecnologias de computação e algoritmos, Random Forest permanece uma ferramenta valiosa no arsenal dos cientistas de dados e engenheiros de machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24257ce6-8266-40c0-94f0-53293b8c4af2",
   "metadata": {},
   "source": [
    "# Parâmetros do `RandomForestClassifier` do scikit-learn:\n",
    "\n",
    "1. `n_estimators`: O número de árvores na floresta.\n",
    "\n",
    "2. `criterion`: A função para medir a qualidade de uma divisão. Pode ser \"gini\" para o índice de Gini ou \"entropy\" para o ganho de informação.\n",
    "\n",
    "3. `max_depth`: A profundidade máxima de cada árvore na floresta.\n",
    "\n",
    "4. `min_samples_split`: O número mínimo de amostras necessárias para dividir um nó interno.\n",
    "\n",
    "5. `min_samples_leaf`: O número mínimo de amostras necessárias para estar em um nó folha.\n",
    "\n",
    "6. `min_weight_fraction_leaf`: A fração mínima ponderada do total de pesos das amostras de entrada necessárias para estar em um nó folha.\n",
    "\n",
    "7. `max_features`: O número de features a serem consideradas ao procurar a melhor divisão. Pode ser um inteiro, \"sqrt\", \"log2\" ou uma fração do total de features.\n",
    "\n",
    "8. `max_leaf_nodes`: O número máximo de folhas permitidas em cada árvore.\n",
    "\n",
    "9. `min_impurity_decrease`: Um nó será dividido se essa divisão induzir uma diminuição da impureza maior ou igual a esse valor.\n",
    "\n",
    "10. `bootstrap`: Se deve amostrar com reposição ao construir árvores.\n",
    "\n",
    "11. `oob_score`: Se deve usar out-of-bag amostras para estimar o erro de generalização.\n",
    "\n",
    "12. `n_jobs`: O número de jobs a serem executados em paralelo para ajustar árvores.\n",
    "\n",
    "13. `random_state`: Determina a semente usada pelo gerador de números aleatórios para garantir que os resultados sejam reproduzíveis.\n",
    "\n",
    "14. `verbose`: Controla a verbosidade da saída durante o ajuste.\n",
    "\n",
    "15. `warm_start`: Se definido como True, reutiliza a solução da chamada anterior para ajustar e adiciona mais árvores à floresta existente.\n",
    "\n",
    "16. `class_weight`: Peso associado a cada classe. Pode ser \"balanced\" para ajustar automaticamente os pesos das classes inversamente proporcionais às frequências das classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28029ac7-ce88-4b06-ba98-9e8658f3f8d5",
   "metadata": {},
   "source": [
    "# Parâmetros do `RandomForestRegressor` do scikit-learn:\n",
    "\n",
    "1. `n_estimators`: O número de árvores na floresta.\n",
    "\n",
    "2. `criterion`: A função para medir a qualidade de uma divisão. Pode ser \"mse\" para o erro médio quadrático ou \"mae\" para o erro absoluto médio.\n",
    "\n",
    "3. `max_depth`: A profundidade máxima de cada árvore na floresta.\n",
    "\n",
    "4. `min_samples_split`: O número mínimo de amostras necessárias para dividir um nó interno.\n",
    "\n",
    "5. `min_samples_leaf`: O número mínimo de amostras necessárias para estar em um nó folha.\n",
    "\n",
    "6. `min_weight_fraction_leaf`: A fração mínima ponderada do total de pesos das amostras de entrada necessárias para estar em um nó folha.\n",
    "\n",
    "7. `max_features`: O número de features a serem consideradas ao procurar a melhor divisão. Pode ser um inteiro, \"sqrt\", \"log2\" ou uma fração do total de features.\n",
    "\n",
    "8. `max_leaf_nodes`: O número máximo de folhas permitidas em cada árvore.\n",
    "\n",
    "9. `min_impurity_decrease`: Um nó será dividido se essa divisão induzir uma diminuição da impureza maior ou igual a esse valor.\n",
    "\n",
    "10. `bootstrap`: Se deve amostrar com reposição ao construir árvores.\n",
    "\n",
    "11. `oob_score`: Se deve usar out-of-bag amostras para estimar o erro de generalização.\n",
    "\n",
    "12. `n_jobs`: O número de jobs a serem executados em paralelo para ajustar árvores.\n",
    "\n",
    "13. `random_state`: Determina a semente usada pelo gerador de números aleatórios para garantir que os resultados sejam reproduzíveis.\n",
    "\n",
    "14. `verbose`: Controla a verbosidade da saída durante o ajuste.\n",
    "\n",
    "15. `warm_start`: Se definido como True, reutiliza a solução da chamada anterior para ajustar e adiciona mais árvores à floresta existente.\n",
    "\n",
    "16. `ccp_alpha`: Parâmetro de complexidade de custo mínimo para poda de custo-complexidade."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc62251-659b-430d-a63d-3e6b29189021",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e31da2-1dec-45ff-bd37-d230c6a5c378",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "readDiag",
   "language": "python",
   "name": "readdiag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
