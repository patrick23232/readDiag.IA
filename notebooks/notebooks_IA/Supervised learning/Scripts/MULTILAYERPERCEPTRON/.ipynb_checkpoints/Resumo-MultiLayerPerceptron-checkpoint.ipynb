{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c61ea48-c66c-4131-b8a1-18fac04da04d",
   "metadata": {},
   "source": [
    "<img src=\"logoINPE.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d649cec6-61b6-4692-a82a-e56a6b533b20",
   "metadata": {},
   "source": [
    "# MultiLayer Perceptron (MLP)\n",
    "\n",
    "**Definição:**\n",
    "O MultiLayer Perceptron (MLP) é um tipo de rede neural artificial que consiste em múltiplas camadas de neurônios, incluindo uma camada de entrada, uma ou mais camadas ocultas e uma camada de saída. É um modelo de aprendizado supervisionado utilizado para tarefas de classificação e regressão. O MLP é um caso específico das redes neurais feedforward, onde a informação flui apenas em uma direção, da entrada para a saída.\n",
    "\n",
    "### Estrutura e Funcionamento\n",
    "\n",
    "**Camadas de um MLP:**\n",
    "\n",
    "1. **Camada de Entrada:**\n",
    "   - A primeira camada que recebe os dados de entrada. Cada neurônio nesta camada representa uma característica dos dados.\n",
    "\n",
    "2. **Camadas Ocultas:**\n",
    "   - Uma ou mais camadas entre a camada de entrada e a camada de saída. Os neurônios nessas camadas realizam a transformação dos dados através de uma combinação linear de pesos, um viés e uma função de ativação não-linear. Essas camadas são responsáveis por capturar padrões e interações complexas nos dados.\n",
    "\n",
    "3. **Camada de Saída:**\n",
    "   - A última camada que produz a previsão final. O número de neurônios nesta camada corresponde ao número de classes (para problemas de classificação) ou ao número de valores a serem previstos (para problemas de regressão).\n",
    "\n",
    "**Neurônios e Pesos:**\n",
    "Cada neurônio em uma camada é conectado a todos os neurônios da camada anterior (conexão densa ou totalmente conectada). Cada conexão possui um peso que é ajustado durante o treinamento.\n",
    "\n",
    "**Função de Ativação:**\n",
    "Funções de ativação introduzem não-linearidade no modelo, permitindo que a rede capture padrões complexos. Exemplos incluem:\n",
    "- **Sigmoide:** \\(\\sigma(x) = \\frac{1}{1 + e^{-x}}\\)\n",
    "- **ReLU (Rectified Linear Unit):** \\(f(x) = \\max(0, x)\\)\n",
    "- **Tanh:** \\(tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\\)\n",
    "\n",
    "### Treinamento do MLP\n",
    "\n",
    "**Algoritmo de Treinamento:**\n",
    "O MLP é treinado utilizando o algoritmo de retropropagação (backpropagation) em conjunto com um otimizador como o Gradiente Descendente. O processo de treinamento envolve os seguintes passos:\n",
    "\n",
    "1. **Feedforward:**\n",
    "   - Os dados de entrada são propagados através da rede, passando pelas camadas de entrada, ocultas e de saída, produzindo uma previsão.\n",
    "\n",
    "2. **Cálculo da Função de Perda:**\n",
    "   - A função de perda mede a discrepância entre as previsões do modelo e os valores reais. Exemplos de funções de perda incluem erro quadrático médio (MSE) para regressão e entropia cruzada para classificação.\n",
    "\n",
    "3. **Retropropagação:**\n",
    "   - O gradiente da função de perda em relação a cada peso é calculado usando a regra da cadeia, começando pela camada de saída e propagando-se para trás através da rede.\n",
    "\n",
    "4. **Atualização dos Pesos:**\n",
    "   - Os pesos são ajustados na direção oposta ao gradiente para minimizar a função de perda. Esta atualização é controlada pela taxa de aprendizado (\\(\\eta\\)).\n",
    "\n",
    "5. **Iteração:**\n",
    "   - O processo é repetido para múltiplas épocas (passagens completas pelo conjunto de treinamento) até que a função de perda atinja um valor aceitável ou convergência.\n",
    "\n",
    "### Parâmetros e Hiperparâmetros\n",
    "\n",
    "1. **Número de Camadas Ocultas:**\n",
    "   - Determina a profundidade da rede. Redes mais profundas podem capturar padrões mais complexos, mas também são mais propensas ao sobreajuste.\n",
    "\n",
    "2. **Número de Neurônios por Camada:**\n",
    "   - Determina a largura da rede. Mais neurônios permitem capturar mais informações, mas também aumentam a complexidade computacional.\n",
    "\n",
    "3. **Taxa de Aprendizado (learning_rate):**\n",
    "   - Controla a magnitude dos ajustes dos pesos. Uma taxa de aprendizado muito alta pode levar a um treinamento instável, enquanto uma taxa muito baixa pode resultar em convergência lenta.\n",
    "\n",
    "4. **Função de Ativação:**\n",
    "   - Escolha da função de ativação para as camadas ocultas. A função ReLU é popular devido à sua simplicidade e eficácia em redes profundas.\n",
    "\n",
    "5. **Número de Épocas (epochs):**\n",
    "   - Número de passagens completas pelo conjunto de treinamento.\n",
    "\n",
    "6. **Tamanho do Lote (batch_size):**\n",
    "   - Número de amostras de treinamento usadas em uma única atualização dos pesos. O treinamento pode ser realizado usando batch (lote completo), mini-batch (pequenos lotes) ou online (uma amostra por vez).\n",
    "\n",
    "### Vantagens do MLP\n",
    "\n",
    "1. **Capacidade de Capturar Padrões Complexos:**\n",
    "   - MLPs podem modelar relações não-lineares complexas nos dados.\n",
    "\n",
    "2. **Flexibilidade:**\n",
    "   - Aplicável a uma ampla gama de tarefas de aprendizado supervisionado, incluindo classificação, regressão e previsão de séries temporais.\n",
    "\n",
    "3. **Escalabilidade:**\n",
    "   - Pode ser escalado para grandes conjuntos de dados e ajustado para diferentes domínios.\n",
    "\n",
    "### Desvantagens do MLP\n",
    "\n",
    "1. **Complexidade Computacional:**\n",
    "   - O treinamento pode ser intensivo em termos de tempo e recursos computacionais, especialmente para redes profundas e grandes conjuntos de dados.\n",
    "\n",
    "2. **Sensibilidade aos Hiperparâmetros:**\n",
    "   - O desempenho pode ser altamente dependente da escolha dos hiperparâmetros, exigindo validação cruzada e experimentação cuidadosa.\n",
    "\n",
    "3. **Tendência ao Sobreajuste:**\n",
    "   - Redes profundas com muitos parâmetros podem sobreajustar dados de treinamento, especialmente se o conjunto de dados for pequeno.\n",
    "\n",
    "### Aplicações do MLP\n",
    "\n",
    "O MLP é amplamente utilizado em diversas áreas devido à sua capacidade de modelar padrões complexos, incluindo:\n",
    "- **Reconhecimento de Imagens:** Classificação de imagens, detecção de objetos.\n",
    "- **Processamento de Linguagem Natural:** Classificação de textos, análise de sentimentos.\n",
    "- **Finanças:** Previsão de preços de ações, análise de risco de crédito.\n",
    "- **Saúde:** Diagnóstico de doenças, análise de dados genômicos.\n",
    "- **Jogos:** Desenvolvimento de agentes inteligentes, como aqueles usados em jogos de tabuleiro e videogames.\n",
    "\n",
    "### Conclusão\n",
    "\n",
    "O MultiLayer Perceptron é uma poderosa técnica de aprendizado supervisionado que forma a base de muitas redes neurais modernas. Com sua capacidade de capturar padrões complexos e suas aplicações em diversas áreas, o MLP continua sendo uma ferramenta essencial para cientistas de dados e engenheiros de machine learning. No entanto, seu uso eficaz requer uma compreensão profunda de sua estrutura, processos de treinamento e ajuste de hiperparâmetros, além de considerações sobre a complexidade computacional e a prevenção do sobreajuste. Com o avanço contínuo na pesquisa e na tecnologia de aprendizado de máquina, os MLPs permanecem uma peça fundamental no desenvolvimento de soluções de inteligência artificial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7121bd-bfc8-4561-9201-f3ba90f66edb",
   "metadata": {},
   "source": [
    "# Parâmetros do `MLPClassifier` e `MlPRegressor` do scikit-learn:\n",
    "\n",
    "1. `hidden_layer_sizes`: Determina a arquitetura da rede neural, especificando o número de neurônios em cada camada oculta. Pode ser uma tupla onde cada elemento representa o número de neurônios em uma camada específica.\n",
    "\n",
    "2. `activation`: Define a função de ativação para as camadas ocultas. As opções incluem \"relu\" (Rectified Linear Unit), \"tanh\" (tangente hiperbólica) e \"logistic\" (função logística).\n",
    "\n",
    "3. `solver`: O otimizador usado para treinar a rede neural. Opções comuns incluem \"adam\", \"lbfgs\" e \"sgd\".\n",
    "\n",
    "4. `alpha`: Parâmetro de regularização que controla a penalização da magnitude dos pesos da rede para evitar overfitting.\n",
    "\n",
    "5. `batch_size`: Tamanho do mini-lote usado durante o treinamento da rede. Isso determina quantos exemplos de treinamento são usados antes de atualizar os pesos da rede.\n",
    "\n",
    "6. `learning_rate`: Determina como a taxa de aprendizagem é adaptada durante o treinamento. Pode ser constante (\"constant\"), adaptativa (\"adaptive\") ou inversa da escala do gradiente (\"invscaling\").\n",
    "\n",
    "7. `learning_rate_init`: A taxa de aprendizagem inicial. Define a taxa na qual os pesos da rede são atualizados durante o treinamento.\n",
    "\n",
    "8. `power_t`: O expoente para a inversão da escala de aprendizado.\n",
    "\n",
    "9. `max_iter`: O número máximo de iterações (épocas) durante o treinamento da rede.\n",
    "\n",
    "10. `shuffle`: Se os dados de treinamento devem ser embaralhados em cada iteração durante o treinamento.\n",
    "\n",
    "11. `random_state`: Determina a semente usada pelo gerador de números aleatórios para garantir que os resultados sejam reproduzíveis.\n",
    "\n",
    "12. `tol`: Tolerância para a convergência do otimizador.\n",
    "\n",
    "13. `verbose`: Se a saída detalhada deve ser impressa durante o treinamento.\n",
    "\n",
    "14. `warm_start`: Se o treinamento deve começar a partir dos pesos anteriores.\n",
    "\n",
    "15. `momentum`: A taxa de momento usada pelo otimizador.\n",
    "\n",
    "16. `nesterovs_momentum`: Se o momento de Nesterov deve ser usado.\n",
    "\n",
    "17. `early_stopping`: Se o treinamento deve parar quando a perda de validação não melhorar.\n",
    "\n",
    "18. `validation_fraction`: A fração dos dados de treinamento a serem reservados como conjunto de validação para a parada antecipada.\n",
    "\n",
    "19. `beta_1`: O parâmetro de decaimento do momento para o otimizador Adam.\n",
    "\n",
    "20. `beta_2`: O parâmetro de decaimento do momento para o otimizador Adam.\n",
    "\n",
    "21. `epsilon`: O valor de ajuste para evitar a divisão por zero no otimizador Adam."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b937628-83f2-4060-b9ac-47cb38dfd06a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "readDiag",
   "language": "python",
   "name": "readdiag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
