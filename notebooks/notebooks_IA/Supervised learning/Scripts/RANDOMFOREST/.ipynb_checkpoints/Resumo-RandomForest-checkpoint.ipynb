{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55a28053-8656-4f72-ac2b-a50574293bfc",
   "metadata": {},
   "source": [
    "<img src=\"logoINPE.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f9ce61-614c-4b5b-9306-bb3f9cd93135",
   "metadata": {},
   "source": [
    "# Floresta Aleatória (Random Forest)\n",
    "\n",
    "A classificação por floresta aleatória é uma técnica de aprendizado de máquina que combina o poder da aleatoriedade com o poder da média para construir um modelo robusto de classificação. Ela opera criando múltiplas árvores de decisão durante o treinamento e fazendo previsões com base na maioria das previsões das árvores individuais.\n",
    "\n",
    "## Principais Características:\n",
    "- **Árvores de Decisão:** Cada árvore na floresta é construída a partir de uma amostra aleatória do conjunto de dados de treinamento. Isso ajuda a reduzir a correlação entre as árvores individuais, tornando o modelo mais robusto.\n",
    "\n",
    "- **Amostragem Aleatória de Características:** Durante a construção de cada árvore, apenas um subconjunto aleatório das características é considerado para dividir em cada nó da árvore. Isso introduz mais diversidade nas árvores e reduz a probabilidade de overfitting.\n",
    "\n",
    "- **Votação por Maioria:** Ao fazer previsões, cada árvore na floresta contribui com uma votação para determinar a classe final de um exemplo. A classe mais frequente entre todas as árvores é escolhida como a previsão final.\n",
    "\n",
    "## Vantagens:\n",
    "- **Robustez:** Averiguando várias árvores de decisão, o modelo é menos propenso a overfitting e tem uma melhor capacidade de generalização em dados de teste.\n",
    "\n",
    "- **Manuseio de Dados Não Lineares e de Alta Dimensão:** Pode lidar eficazmente com conjuntos de dados com muitas características e interações complexas entre elas.\n",
    "\n",
    "- **Fácil de Usar:** Requer pouca ou nenhuma sintonia de parâmetros e lida bem com dados ausentes.\n",
    "\n",
    "## Limitações:\n",
    "- **Interpretabilidade:** Às vezes, a floresta aleatória pode ser difícil de interpretar em comparação com modelos lineares simples.\n",
    "\n",
    "- **Desempenho em Dados Escaláveis:** Para conjuntos de dados muito grandes, o treinamento de uma floresta aleatória pode se tornar computacionalmente caro.\n",
    "\n",
    "## Aplicações:\n",
    "- **Classificação:** Prever a classe de um exemplo com base em suas características.\n",
    "\n",
    "A implementação da classificação por floresta aleatória no scikit-learn, utilizando RandomForestClassifier, oferece uma interface simples e poderosa para treinar e fazer previsões com este modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24257ce6-8266-40c0-94f0-53293b8c4af2",
   "metadata": {},
   "source": [
    "# Parâmetros do `RandomForestClassifier` do scikit-learn:\n",
    "\n",
    "1. `n_estimators`: O número de árvores na floresta.\n",
    "\n",
    "2. `criterion`: A função para medir a qualidade de uma divisão. Pode ser \"gini\" para o índice de Gini ou \"entropy\" para o ganho de informação.\n",
    "\n",
    "3. `max_depth`: A profundidade máxima de cada árvore na floresta.\n",
    "\n",
    "4. `min_samples_split`: O número mínimo de amostras necessárias para dividir um nó interno.\n",
    "\n",
    "5. `min_samples_leaf`: O número mínimo de amostras necessárias para estar em um nó folha.\n",
    "\n",
    "6. `min_weight_fraction_leaf`: A fração mínima ponderada do total de pesos das amostras de entrada necessárias para estar em um nó folha.\n",
    "\n",
    "7. `max_features`: O número de features a serem consideradas ao procurar a melhor divisão. Pode ser um inteiro, \"sqrt\", \"log2\" ou uma fração do total de features.\n",
    "\n",
    "8. `max_leaf_nodes`: O número máximo de folhas permitidas em cada árvore.\n",
    "\n",
    "9. `min_impurity_decrease`: Um nó será dividido se essa divisão induzir uma diminuição da impureza maior ou igual a esse valor.\n",
    "\n",
    "10. `bootstrap`: Se deve amostrar com reposição ao construir árvores.\n",
    "\n",
    "11. `oob_score`: Se deve usar out-of-bag amostras para estimar o erro de generalização.\n",
    "\n",
    "12. `n_jobs`: O número de jobs a serem executados em paralelo para ajustar árvores.\n",
    "\n",
    "13. `random_state`: Determina a semente usada pelo gerador de números aleatórios para garantir que os resultados sejam reproduzíveis.\n",
    "\n",
    "14. `verbose`: Controla a verbosidade da saída durante o ajuste.\n",
    "\n",
    "15. `warm_start`: Se definido como True, reutiliza a solução da chamada anterior para ajustar e adiciona mais árvores à floresta existente.\n",
    "\n",
    "16. `class_weight`: Peso associado a cada classe. Pode ser \"balanced\" para ajustar automaticamente os pesos das classes inversamente proporcionais às frequências das classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28029ac7-ce88-4b06-ba98-9e8658f3f8d5",
   "metadata": {},
   "source": [
    "# Parâmetros do `RandomForestRegressor` do scikit-learn:\n",
    "\n",
    "1. `n_estimators`: O número de árvores na floresta.\n",
    "\n",
    "2. `criterion`: A função para medir a qualidade de uma divisão. Pode ser \"mse\" para o erro médio quadrático ou \"mae\" para o erro absoluto médio.\n",
    "\n",
    "3. `max_depth`: A profundidade máxima de cada árvore na floresta.\n",
    "\n",
    "4. `min_samples_split`: O número mínimo de amostras necessárias para dividir um nó interno.\n",
    "\n",
    "5. `min_samples_leaf`: O número mínimo de amostras necessárias para estar em um nó folha.\n",
    "\n",
    "6. `min_weight_fraction_leaf`: A fração mínima ponderada do total de pesos das amostras de entrada necessárias para estar em um nó folha.\n",
    "\n",
    "7. `max_features`: O número de features a serem consideradas ao procurar a melhor divisão. Pode ser um inteiro, \"sqrt\", \"log2\" ou uma fração do total de features.\n",
    "\n",
    "8. `max_leaf_nodes`: O número máximo de folhas permitidas em cada árvore.\n",
    "\n",
    "9. `min_impurity_decrease`: Um nó será dividido se essa divisão induzir uma diminuição da impureza maior ou igual a esse valor.\n",
    "\n",
    "10. `bootstrap`: Se deve amostrar com reposição ao construir árvores.\n",
    "\n",
    "11. `oob_score`: Se deve usar out-of-bag amostras para estimar o erro de generalização.\n",
    "\n",
    "12. `n_jobs`: O número de jobs a serem executados em paralelo para ajustar árvores.\n",
    "\n",
    "13. `random_state`: Determina a semente usada pelo gerador de números aleatórios para garantir que os resultados sejam reproduzíveis.\n",
    "\n",
    "14. `verbose`: Controla a verbosidade da saída durante o ajuste.\n",
    "\n",
    "15. `warm_start`: Se definido como True, reutiliza a solução da chamada anterior para ajustar e adiciona mais árvores à floresta existente.\n",
    "\n",
    "16. `ccp_alpha`: Parâmetro de complexidade de custo mínimo para poda de custo-complexidade."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc62251-659b-430d-a63d-3e6b29189021",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e31da2-1dec-45ff-bd37-d230c6a5c378",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "readDiag",
   "language": "python",
   "name": "readdiag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
